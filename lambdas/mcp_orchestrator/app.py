import json
import boto3
import os
import re
import time
import hashlib
from datetime import datetime, timedelta
import pytz
import logging
from typing import Dict, List, Any
import asyncio
import urllib.request
import urllib.error
from urllib.parse import urljoin, urlparse, quote
from pathlib import Path

logger = logging.getLogger()
logger.setLevel(logging.INFO)

s3_client = boto3.client('s3')
lambda_client = boto3.client('lambda')

class BaystarsRSSNewsCollector:
    """Google News RSSçµŒç”±ã§ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã™ã‚‹é«˜é€Ÿã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        import urllib.request
        self.session_headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã®ãƒãƒƒã‚·ãƒ¥ä¿å­˜ï¼ˆLambdaç’°å¢ƒã§ã¯ä¸€æ™‚çš„ï¼‰
        self.seen_articles = set()
        
        # æ¨ã—é¸æ‰‹ãƒªã‚¹ãƒˆï¼ˆå€‹åˆ¥æ¤œç´¢ç”¨ï¼‰
        self.featured_players = []
        
        # å…¨ä½“æ¤œç´¢ã‚¯ã‚¨ãƒª
        self.general_queries = [
            "æ¨ªæµœDeNAãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º",
            "æ¨ªæµœDeNAãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º è©¦åˆ"
        ]
        
        # æ—§ã‚·ã‚¹ãƒ†ãƒ äº’æ›æ€§ã®ãŸã‚
        self.search_queries = self.general_queries
        
        # ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        self.baystars_keywords = [
            'ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º', 'DeNA', 'æ¨ªæµœ', 'baystars', 'dena', 'yokohama',
            'ä½é‡', 'ç‰§', 'å®®å´', 'æ£®', 'é–¢æ ¹', 'æ¡‘åŸ', 'å¤§å’Œ', 'ä¸‰æµ¦',
            'ä»Šæ°¸', 'å¤§è²«', 'æ±', 'ä¸ŠèŒ¶è°·', 'å¹³è‰¯', 'å…¥æ±Ÿ', 'ä¼Šå‹¢', 'å±±å´',
            'æˆ¸æŸ±', 'ç›Šå­', 'è¦å', 'äº¬ç”°', 'æ¾å°¾', 'çŸ³å·',
            'è—¤æµª', 'ãƒ“ã‚·ã‚¨ãƒ‰', 'æ©‹æœ¬é”å¼¥'
        ]

    def _generate_article_hash(self, url, title):
        """è¨˜äº‹ã®ä¸€æ„ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ"""
        content = f"{url}#{title}".encode('utf-8')
        return hashlib.md5(content).hexdigest()

    def _is_baystars_related(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆãŒãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£ã‹ãƒã‚§ãƒƒã‚¯"""
        if not text:
            return False
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in self.baystars_keywords)

    def fetch_rss_feed(self, query):
        """Google News RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—"""
        try:
            # Google News RSS URLæ§‹ç¯‰
            encoded_query = quote(query)
            rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ja&gl=JP&ceid=JP:ja"
            
            logger.info(f"ğŸ“¡ RSSå–å¾—ä¸­: {query}")
            
            # feedparserãŒãªã„å ´åˆã¯ urllib ã§ä»£æ›¿å®Ÿè£…
            try:
                import feedparser
                feed = feedparser.parse(rss_url)
                if feed.bozo:
                    logger.warning(f"âš ï¸  RSSè§£æè­¦å‘Š: {feed.bozo_exception}")
                return feed
            except ImportError:
                # feedparserãŒãªã„å ´åˆã®ä»£æ›¿å®Ÿè£…
                logger.warning("feedparser not available, using fallback RSS parsing")
                return self._parse_rss_manually(rss_url)
            
        except Exception as e:
            logger.error(f"âŒ RSSå–å¾—ã‚¨ãƒ©ãƒ¼ ({query}): {e}")
            return None

    def _parse_rss_manually(self, rss_url):
        """feedparserãŒä½¿ç”¨ã§ããªã„å ´åˆã®æ‰‹å‹•RSSè§£æ"""
        try:
            import urllib.request
            import xml.etree.ElementTree as ET
            
            req = urllib.request.Request(rss_url, headers=self.session_headers)
            with urllib.request.urlopen(req, timeout=10) as response:
                xml_content = response.read().decode('utf-8')
            
            # ç°¡å˜ãªXMLè§£æ
            root = ET.fromstring(xml_content)
            
            # RSS ã‚¨ãƒ³ãƒˆãƒªã‚’æŠ½å‡º
            entries = []
            for item in root.findall('.//item'):
                entry = {}
                title_elem = item.find('title')
                link_elem = item.find('link')
                desc_elem = item.find('description')
                pub_elem = item.find('pubDate')
                
                if title_elem is not None:
                    entry['title'] = title_elem.text
                if link_elem is not None:
                    entry['link'] = link_elem.text
                if desc_elem is not None:
                    entry['summary'] = desc_elem.text
                if pub_elem is not None:
                    entry['published'] = pub_elem.text
                    
                # ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’æŠ½å‡ºï¼ˆå¯èƒ½ã§ã‚ã‚Œã°ï¼‰
                source_elem = item.find('.//{http://search.yahoo.com/mrss/}credit')
                if source_elem is not None:
                    entry['source'] = {'title': source_elem.text}
                else:
                    entry['source'] = {'title': 'Google News'}
                
                entries.append(entry)
            
            # feedparserãƒ©ã‚¤ã‚¯ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
            class FeedLike:
                def __init__(self, entries):
                    self.entries = entries
                    self.bozo = False
            
            return FeedLike(entries)
            
        except Exception as e:
            logger.error(f"Manual RSS parsing failed: {e}")
            return None

    def extract_article_from_rss_item(self, item):
        """RSS itemã‹ã‚‰è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡º"""
        try:
            # åŸºæœ¬æƒ…å ±ã‚’RSSã‹ã‚‰å–å¾—
            article_data = {
                'title': item.get('title', ''),
                'url': item.get('link', ''),
                'summary': item.get('summary', ''),
                'publish_date': item.get('published', ''),
                'source': item.get('source', {}).get('title', 'Google News'),
                'scraped_at': datetime.now().isoformat(),
                'content': '',
                'images': [],
                'is_baystars_related': False,
                'baystars_keywords_found': [],
                'collection_method': 'google_news_rss',
                'player_relevance_score': 0,
                'is_player_focused': False
            }
            
            # HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã¦ç´”ç²‹ãªãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            if article_data['summary']:
                # ç°¡å˜ãªHTMLã‚¿ã‚°é™¤å»
                import re
                article_data['summary'] = re.sub(r'<[^>]*>', '', article_data['summary']).strip()
            
            # ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£åˆ¤å®š
            full_text = f"{article_data['title']} {article_data['summary']}"
            article_data['is_baystars_related'] = self._is_baystars_related(full_text)
            
            # è¦‹ã¤ã‹ã£ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨˜éŒ²
            found_keywords = []
            for keyword in self.baystars_keywords:
                if keyword.lower() in full_text.lower():
                    found_keywords.append(keyword)
            article_data['baystars_keywords_found'] = found_keywords
            
            # é¸æ‰‹é–¢é€£åº¦ã®åˆ¤å®šã‚’è¿½åŠ 
            player_score, is_player_focused = self._assess_player_relevance(full_text)
            article_data['player_relevance_score'] = player_score
            article_data['is_player_focused'] = is_player_focused
            
            return article_data
            
        except Exception as e:
            logger.error(f"âŒ RSSè¨˜äº‹æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _assess_player_relevance(self, text):
        """è¨˜äº‹ã®é¸æ‰‹é–¢é€£åº¦ã‚’è©•ä¾¡"""
        
        # é¸æ‰‹åãƒªã‚¹ãƒˆï¼ˆã‚ˆã‚ŠåŒ…æ‹¬çš„ï¼‰
        player_names = [
            # ç¾å½¹é¸æ‰‹åï¼ˆè‹—å­—ã®ã¿ã§ã‚‚æ¤œå‡ºï¼‰
            'ä½é‡', 'ç‰§', 'å®®å´', 'äº¬ç”°', 'æ¾å°¾', 'æˆ¸æŸ±', 'æ¡‘åŸ', 'ç¥é‡Œ', 'é–¢æ ¹', 'è¦å',
            'æ±', 'å¤§è²«', 'ãƒã‚¦ã‚¢ãƒ¼', 'ã‚¸ãƒ£ã‚¯ã‚½ãƒ³', 'ä¼Šå‹¢', 'å±±å´', 'å…¥æ±Ÿ', 'æ£®åŸ', 'çŸ³ç”°',
            'æŸ´ç”°', 'äº•ä¸Š', 'ãƒ•ã‚©ãƒ¼ãƒ‰', 'æ—', 'çŸ³ä¸Š',
            # ãƒ•ãƒ«ãƒãƒ¼ãƒ 
            'ä½é‡æµå¤ª', 'ç‰§ç§€æ‚Ÿ', 'å®®å´æ•éƒ', 'äº¬ç”°é™½å¤ª', 'æ¾å°¾æ±æ©', 'æˆ¸æŸ±æ­å­',
            'æ¡‘åŸå°†å¿—', 'ç¥é‡Œå’Œæ¯…', 'é–¢æ ¹å¤§æ°—', 'è¦åé”å¤«',
            'æ±å…‹æ¨¹', 'å¤§è²«æ™‹ä¸€', 'ä¼Šå‹¢å¤§å¤¢', 'å±±å´åº·æ™ƒ'
        ]
        
        # é¸æ‰‹é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        player_keywords = [
            'æ´»èº', 'æ‰“æ’ƒ', 'æŠ•çƒ', 'å®ˆå‚™', 'å¥½èª¿', 'ä¸èª¿', 'å¾©æ´»', 'ãƒ›ãƒ¼ãƒ ãƒ©ãƒ³', 'ãƒ’ãƒƒãƒˆ',
            'å…ˆç™º', 'ç™»æ¿', 'å‹åˆ©', 'æ•—æˆ¦', 'å¥½æŠ•', 'å®ŒæŠ•', 'æ•‘æ´', 'ã‚»ãƒ¼ãƒ–', 'å¥ªä¸‰æŒ¯',
            'æ‰“ç‚¹', 'å¾—ç‚¹', 'ç›—å¡', 'ä½µæ®º', 'ã‚¨ãƒ©ãƒ¼', 'å¥½å®ˆ', 'é€çƒ', 'ã‚­ãƒ£ãƒƒãƒãƒ³ã‚°',
            'èª¿å­', 'æˆç¸¾', 'æ‰“ç‡', 'é˜²å¾¡ç‡', 'å‹ç‡', 'OPS', 'ERA',
            'ç·´ç¿’', 'ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°', 'ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ³', 'æ€ªæˆ‘', 'å¾©å¸°', 'é›¢è„±',
            'ç§»ç±', 'ãƒˆãƒ¬ãƒ¼ãƒ‰', 'å¥‘ç´„', 'å¹´ä¿¸', 'FA', 'æ®‹ç•™', 'ç²å¾—', 'æ”¾å‡º'
        ]
        
        # éé¸æ‰‹é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã“ã‚Œã‚‰ãŒã‚ã‚‹ã¨æ¸›ç‚¹ï¼‰
        non_player_keywords = [
            'ã‚¹ãƒãƒ³ã‚µãƒ¼', 'å”è³›', 'ä¼æ¥­', 'å¥‘ç´„', 'ææº', 'ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚·ãƒƒãƒ—',
            'ã‚¤ãƒ™ãƒ³ãƒˆ', 'ã‚°ãƒƒã‚º', 'å•†å“', 'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³', 'è²©å£²', 'ä¾¡æ ¼',
            'ãƒ•ã‚¡ãƒ³ã‚¯ãƒ©ãƒ–', 'ãƒã‚±ãƒƒãƒˆ', 'å…¥å ´', 'è¦³å®¢', 'å‹•å“¡', 'å£²ä¸Š',
            'çƒå›£', 'çµŒå–¶', 'é‹å–¶', 'ç¤¾é•·', 'ç›£ç£äººäº‹', 'ã‚³ãƒ¼ãƒ', 'ã‚¹ã‚¿ãƒƒãƒ•',
            'æ–½è¨­', 'ã‚¹ã‚¿ã‚¸ã‚¢ãƒ ', 'æ”¹ä¿®', 'å·¥äº‹', 'å»ºè¨­', 'ã‚¢ã‚¯ã‚»ã‚¹'
        ]
        
        score = 0
        text_lower = text.lower()
        
        # é¸æ‰‹åã®æ¤œå‡ºï¼ˆé«˜é…ç‚¹ï¼‰
        for player in player_names:
            if player in text:
                score += 3  # é¸æ‰‹åã¯é«˜å¾—ç‚¹
        
        # é¸æ‰‹é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ¤œå‡º
        for keyword in player_keywords:
            if keyword in text:
                score += 1
        
        # éé¸æ‰‹é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ¤œå‡ºï¼ˆæ¸›ç‚¹ï¼‰
        for keyword in non_player_keywords:
            if keyword in text:
                score -= 2
        
        # è©¦åˆé–¢é€£ã‹ã©ã†ã‹ã®åˆ¤å®šï¼ˆé¸æ‰‹ã¨é–¢é€£ã—ã‚„ã™ã„ï¼‰
        game_keywords = ['è©¦åˆ', 'å¯¾æˆ¦', 'å‹åˆ©', 'æ•—æˆ¦', 'å‹è² ', 'æˆ¦ã„', 'vs', 'æˆ¦']
        for keyword in game_keywords:
            if keyword in text:
                score += 1
        
        # é¸æ‰‹é‡è¦–åˆ¤å®š
        is_player_focused = score >= 3  # 3ç‚¹ä»¥ä¸Šã§é¸æ‰‹é–¢é€£ã¨ã¿ãªã™
        
        return max(0, score), is_player_focused

    def set_featured_players(self, players):
        """æ¨ã—é¸æ‰‹ã‚’è¨­å®š"""
        self.featured_players = players
        logger.info(f"ğŸ¯ æ¨ã—é¸æ‰‹è¨­å®š: {', '.join(players)}")

    def collect_player_specific_news(self, player_name, max_articles=5):
        """ç‰¹å®šé¸æ‰‹ã®å€‹åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        logger.info(f"ğŸ” {player_name} é¸æ‰‹ã®å€‹åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢é–‹å§‹...")
        
        # é¸æ‰‹åã§ã®æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆ
        player_queries = [
            f"{player_name} æ¨ªæµœDeNA",
            f"{player_name} ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º",
            f"{player_name}"
        ]
        
        player_articles = []
        
        for query in player_queries:
            logger.info(f"   ğŸ“‹ ã‚¯ã‚¨ãƒª: {query}")
            
            feed = self.fetch_rss_feed(query)
            if not feed or not hasattr(feed, 'entries'):
                continue
            
            logger.info(f"   ğŸ“Š RSSè¨˜äº‹æ•°: {len(feed.entries)} ä»¶")
            
            for i, item in enumerate(feed.entries[:max_articles], 1):
                title = item.get('title', '')
                url = item.get('link', '')
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                article_hash = self._generate_article_hash(url, title)
                if article_hash in self.seen_articles:
                    logger.info(f"   [{i:2d}] â­ï¸  ã‚¹ã‚­ãƒƒãƒ— (æ—¢å‡¦ç†): {title[:40]}...")
                    continue
                
                # ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£ + å¯¾è±¡é¸æ‰‹è¨€åŠãƒã‚§ãƒƒã‚¯
                article_data = self.extract_article_from_rss_item(item)
                if article_data and self._is_player_mentioned(article_data, player_name):
                    self.seen_articles.add(article_hash)
                    article_data['hash'] = article_hash
                    article_data['query_used'] = query
                    article_data['target_player'] = player_name
                    article_data['search_type'] = 'player_specific'
                    
                    player_articles.append(article_data)
                    
                    logger.info(f"   [{i:2d}] âœ… {player_name}é¸æ‰‹é–¢é€£è¨˜äº‹ã‚’åé›†")
                    logger.info(f"        ğŸ“° {title[:50]}...")
                    logger.info(f"        ğŸ·ï¸  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(article_data['baystars_keywords_found'][:3])}")
                    
                    # é¸æ‰‹å€‹åˆ¥æ¤œç´¢ã§ã¯å°‘æ•°ç²¾é‹­ã§
                    if len(player_articles) >= 3:
                        break
            
            # ååˆ†ãªè¨˜äº‹ãŒè¦‹ã¤ã‹ã£ãŸã‚‰æ¬¡ã®ã‚¯ã‚¨ãƒªã¯ã‚¹ã‚­ãƒƒãƒ—
            if len(player_articles) >= 3:
                break
            
            time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
        
        logger.info(f"   ğŸ“Š {player_name}é¸æ‰‹ã®è¨˜äº‹: {len(player_articles)} ä»¶åé›†")
        return player_articles

    def _is_player_mentioned(self, article_data, player_name):
        """è¨˜äº‹ã«ç‰¹å®šé¸æ‰‹ãŒè¨€åŠã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        if not article_data['is_baystars_related']:
            return False
        
        full_text = f"{article_data['title']} {article_data['summary']}"
        
        # é¸æ‰‹åã®æ§˜ã€…ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        name_patterns = [
            player_name,  # ãƒ•ãƒ«ãƒãƒ¼ãƒ 
            player_name.split()[0] if ' ' in player_name else player_name,  # è‹—å­—ã®ã¿
        ]
        
        # è‹—å­—ã ã‘ã®å ´åˆã‚‚è¿½åŠ 
        if len(player_name) > 2:
            name_patterns.append(player_name[:2])  # æœ€åˆã®2æ–‡å­—ï¼ˆè‹—å­—ï¼‰
        
        for pattern in name_patterns:
            if pattern in full_text:
                return True
        
        return False

    def collect_general_baystars_news(self, max_articles=5):
        """ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºå…¨ä½“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        logger.info(f"ğŸ”„ ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºå…¨ä½“ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹...")
        
        general_articles = []
        
        for query in self.general_queries:
            logger.info(f"ğŸ“‹ å…¨ä½“ã‚¯ã‚¨ãƒª: {query}")
            
            feed = self.fetch_rss_feed(query)
            if not feed or not hasattr(feed, 'entries'):
                continue
            
            logger.info(f"   ğŸ“Š RSSè¨˜äº‹æ•°: {len(feed.entries)} ä»¶")
            
            for i, item in enumerate(feed.entries[:max_articles], 1):
                title = item.get('title', '')
                url = item.get('link', '')
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                article_hash = self._generate_article_hash(url, title)
                if article_hash in self.seen_articles:
                    continue
                
                article_data = self.extract_article_from_rss_item(item)
                if article_data and article_data['is_baystars_related']:
                    self.seen_articles.add(article_hash)
                    article_data['hash'] = article_hash
                    article_data['query_used'] = query
                    article_data['search_type'] = 'general_team'
                    
                    general_articles.append(article_data)
                    
                    logger.info(f"   [{i:2d}] âœ… å…¨ä½“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†")
                    logger.info(f"        ğŸ“° {title[:50]}...")
            
            time.sleep(1)
        
        logger.info(f"ğŸ“Š å…¨ä½“ãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(general_articles)} ä»¶åé›†")
        return general_articles

    def collect_comprehensive_news(self, featured_players, max_per_player=3, max_general=5):
        """æ¨ã—é¸æ‰‹ + å…¨ä½“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®åŒ…æ‹¬çš„åé›†"""
        logger.info(f"ğŸ¯ åŒ…æ‹¬çš„ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹ (æ¨ã—é¸æ‰‹: {len(featured_players)}å, å…¨ä½“ãƒ‹ãƒ¥ãƒ¼ã‚¹: {max_general}ä»¶)")
        
        self.set_featured_players(featured_players)
        
        all_articles = []
        player_articles_by_name = {}
        
        # 1. æ¨ã—é¸æ‰‹å€‹åˆ¥æ¤œç´¢
        for player in featured_players:
            player_articles = self.collect_player_specific_news(player, max_per_player)
            player_articles_by_name[player] = player_articles
            all_articles.extend(player_articles)
        
        # 2. ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºå…¨ä½“æ¤œç´¢
        general_articles = self.collect_general_baystars_news(max_general)
        all_articles.extend(general_articles)
        
        # çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
        logger.info(f"\nğŸ“Š åŒ…æ‹¬çš„åé›†çµæœ:")
        logger.info(f"   ğŸ¯ æ¨ã—é¸æ‰‹è¨˜äº‹: {sum(len(articles) for articles in player_articles_by_name.values())} ä»¶")
        for player, articles in player_articles_by_name.items():
            logger.info(f"     - {player}: {len(articles)} ä»¶")
        logger.info(f"   ğŸ“° å…¨ä½“è¨˜äº‹: {len(general_articles)} ä»¶")
        logger.info(f"   ğŸ“Š åˆè¨ˆ: {len(all_articles)} ä»¶")
        
        return {
            'all_articles': all_articles,
            'player_articles': player_articles_by_name,
            'general_articles': general_articles,
            'summary': {
                'total_articles': len(all_articles),
                'player_articles_count': sum(len(articles) for articles in player_articles_by_name.values()),
                'general_articles_count': len(general_articles),
                'featured_players': featured_players
            }
        }

    def collect_news_from_multiple_queries(self, max_articles_per_query=10):
        """è¤‡æ•°ã®ã‚¯ã‚¨ãƒªã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
        logger.info(f"ğŸ”„ {len(self.search_queries)} ç¨®é¡ã®ã‚¯ã‚¨ãƒªã§RSSãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹...")
        
        all_articles = []
        new_articles_count = 0
        
        for i, query in enumerate(self.search_queries, 1):
            logger.info(f"ğŸ“‹ ã‚¯ã‚¨ãƒª {i}/{len(self.search_queries)}: {query}")
            
            # RSSãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—
            feed = self.fetch_rss_feed(query)
            if not feed or not hasattr(feed, 'entries'):
                logger.info(f"   âŒ ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—å¤±æ•—")
                continue
            
            logger.info(f"   ğŸ“Š RSSè¨˜äº‹æ•°: {len(feed.entries)} ä»¶")
            
            query_articles = []
            
            for j, item in enumerate(feed.entries[:max_articles_per_query], 1):
                title = item.get('title', '')
                url = item.get('link', '')
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                article_hash = self._generate_article_hash(url, title)
                if article_hash in self.seen_articles:
                    logger.info(f"   [{j:2d}] â­ï¸  ã‚¹ã‚­ãƒƒãƒ— (æ—¢å‡¦ç†): {title[:40]}...")
                    continue
                
                logger.info(f"   [{j:2d}] ğŸ†• å‡¦ç†ä¸­: {title[:40]}...")
                
                # RSSè¨˜äº‹ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
                article_data = self.extract_article_from_rss_item(item)
                
                if article_data and article_data['is_baystars_related']:
                    # æ–°ã—ã„è¨˜äº‹ã¨ã—ã¦è¨˜éŒ²
                    self.seen_articles.add(article_hash)
                    article_data['hash'] = article_hash
                    article_data['query_used'] = query
                    
                    query_articles.append(article_data)
                    all_articles.append(article_data)
                    new_articles_count += 1
                    
                    # é¸æ‰‹é–¢é€£åº¦ã«ã‚ˆã‚‹ãƒ­ã‚°è¡¨ç¤º
                    player_status = "ğŸ† é¸æ‰‹ä¸­å¿ƒ" if article_data['is_player_focused'] else "ğŸ“° ä¸€èˆ¬"
                    logger.info(f"        âœ… ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£è¨˜äº‹ã¨ã—ã¦åé›† ({player_status})")
                    logger.info(f"        ğŸ·ï¸  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(article_data['baystars_keywords_found'][:3])}")
                    logger.info(f"        ğŸ“Š é¸æ‰‹é–¢é€£åº¦: {article_data['player_relevance_score']}ç‚¹")
                    logger.info(f"        ğŸ“° ã‚½ãƒ¼ã‚¹: {article_data['source']}")
                    
                else:
                    logger.info(f"        â­ï¸  ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£ã§ã¯ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            
            logger.info(f"   ğŸ“Š ã‚¯ã‚¨ãƒªçµæœ: {len(query_articles)} ä»¶ã®æ–°è¨˜äº‹")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆGoogle News APIã«å„ªã—ãï¼‰
            time.sleep(1)
        
        logger.info(f"ğŸ“Š RSSåé›†å®Œäº†: {new_articles_count} ä»¶ã®æ–°ã—ã„ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºè¨˜äº‹ã‚’å–å¾—")
        return all_articles

    def generate_summary_report(self, articles):
        """åé›†çµæœã®ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if not articles:
            return "ğŸ“­ æ–°ã—ã„è¨˜äº‹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
        
        # çµ±è¨ˆæƒ…å ±
        total_articles = len(articles)
        sources = {}
        keywords_count = {}
        
        for article in articles:
            # ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ
            source = article.get('source', 'Unknown')
            sources[source] = sources.get(source, 0) + 1
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¥çµ±è¨ˆ
            for keyword in article.get('baystars_keywords_found', []):
                keywords_count[keyword] = keywords_count.get(keyword, 0) + 1
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = [
            f"ğŸ“Š ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºRSSãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ãƒ¬ãƒãƒ¼ãƒˆ",
            f"ğŸ¯ æ–°è¦è¨˜äº‹æ•°: {total_articles} ä»¶",
            f"ğŸ“° æƒ…å ±ã‚½ãƒ¼ã‚¹æ•°: {len(sources)} ã‚µã‚¤ãƒˆ",
        ]
        
        # ã‚½ãƒ¼ã‚¹åˆ¥è¡¨ç¤ºï¼ˆä¸Šä½5ã¤ï¼‰
        if sources:
            report.append("ğŸ“ˆ ä¸»è¦ã‚½ãƒ¼ã‚¹:")
            sorted_sources = sorted(sources.items(), key=lambda x: x[1], reverse=True)
            for source, count in sorted_sources[:5]:
                report.append(f"   {source}: {count} ä»¶")
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¥è¡¨ç¤ºï¼ˆä¸Šä½5å€‹ï¼‰
        if keywords_count:
            report.append("ğŸ·ï¸  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:")
            sorted_keywords = sorted(keywords_count.items(), key=lambda x: x[1], reverse=True)
            for keyword, count in sorted_keywords[:5]:
                report.append(f"   {keyword}: {count} å›")
        
        return "\n".join(report)

class MCPAgent:
    """MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, name: str):
        self.name = name
        self.logger = logging.getLogger(f"MCP.{name}")
    
    async def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰"""
        raise NotImplementedError

class DataCollectionAgent(MCPAgent):
    """ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self):
        super().__init__("DataCollection")
    
    async def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """è©¦åˆæƒ…å ±ã¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
        
        self.logger.info("Starting data collection...")
        
        # æ™‚é–“å¸¯ã‚’åˆ¤å®š
        jst = pytz.timezone('Asia/Tokyo')
        now_jst = datetime.now(jst)
        time_period = "morning" if 6 <= now_jst.hour <= 12 else "evening"
        
        # 1. è©¦åˆæƒ…å ±åé›†
        game_info = await self._fetch_game_info()
        
        # 2. é¸æ‰‹æƒ…å ±æ›´æ–°ï¼ˆæ™‚é–“å¸¯åˆ¥ï¼‰
        player_info = await self._fetch_current_players(time_period)
        
        # 3. ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±åé›†
        news_info = await self._fetch_news()
        
        collected_data = {
            'game_info': game_info,
            'player_info': player_info,
            'news_info': news_info,
            'collection_timestamp': datetime.now().isoformat()
        }
        
        self.logger.info(f"Data collection completed: {len(collected_data)} items")
        
        return {
            'status': 'success',
            'data': collected_data,
            'agent': self.name
        }
    
    async def _fetch_game_info(self) -> Dict[str, Any]:
        """Yahoo!ã‚¹ãƒãƒ¼ãƒ„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¤œç´¢ãƒ»åˆ†æ"""
        
        try:
            # Yahoo!ã‚¹ãƒãƒ¼ãƒ„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§æœ€æ–°ã®ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†
            news_info = await self._search_baystars_news()
            
            return news_info
            
        except Exception as e:
            self.logger.error(f"Error fetching recent news: {e}")
            return self._get_fallback_game_info()
    
    async def _fetch_current_players(self, time_period: str = None) -> Dict[str, Any]:
        """ç¾åœ¨ã®é¸æ‰‹æƒ…å ±ã‚’å–å¾—ï¼ˆ2025å¹´å®Ÿéš›ã®ãƒ­ã‚¹ã‚¿ãƒ¼ï¼‰"""
        
        # 2025å¹´ã‚·ãƒ¼ã‚ºãƒ³ã®å®Ÿéš›ã®ä¸€è»ç™»éŒ²é¸æ‰‹
        current_players = {
            'pitchers': ['æ±å…‹æ¨¹', 'ä¼Šå‹¢å¤§å¤¢', 'å¤§è²«æ™‹ä¸€', 'ãƒã‚¦ã‚¢ãƒ¼', 'ã‚¸ãƒ£ã‚¯ã‚½ãƒ³', 'å®®åŸæ»å¤ª', 'å‚æœ¬è£•å“‰', 'æ£®åŸåº·å¹³', 'é¢¯', 'çŸ³ç”°è£•å¤ªéƒ', 'ä¸­å·è™å¤§', 'æ¾æœ¬å‡Œäºº', 'è‹¥æ¾å°šè¼', 'å…¥æ±Ÿå¤§ç”Ÿ', 'ã‚±ã‚¤', 'ã‚¦ã‚£ãƒƒã‚¯'],
            'catchers': ['æ¾å°¾æ±æ©', 'æˆ¸æŸ±æ­å­', 'å±±æœ¬ç¥å¤§'],
            'infielders': ['ç‰§ç§€æ‚Ÿ', 'å®®ï¨‘æ•éƒ', 'äº¬ç”°é™½å¤ª', 'äº•ä¸Šçµ¢ç™»', 'ãƒ•ã‚©ãƒ¼ãƒ‰', 'æŸ´ç”°ç«œæ‹“', 'æ—ç¢çœŸ', 'çŸ³ä¸Šæ³°è¼'],
            'outfielders': ['ä½é‡æµå¤ª', 'æ¡‘åŸå°†å¿—', 'ç¥é‡Œå’Œæ¯…', 'é–¢æ ¹å¤§æ°—', 'è¦åé”å¤«', 'ç­’é¦™å˜‰æ™º']
        }
        
        # ã‚¹ãƒãƒ¼ãƒˆé¸æ‰‹é¸æŠï¼ˆæ™‚é–“å¸¯åˆ¥å¯¾å¿œï¼‰
        selected_players = self._select_featured_players(current_players, time_period)
        
        return {
            'roster': current_players,
            'featured_players': selected_players,
            'last_updated': datetime.now().isoformat()
        }
    
    def _select_featured_players(self, roster: Dict[str, List[str]], time_period: str = None) -> List[str]:
        """ä»Šæ—¥æ³¨ç›®ã™ã‚‹é¸æ‰‹ã‚’ã‚¹ãƒãƒ¼ãƒˆã«é¸æŠï¼ˆ3åã€æ™‚é–“å¸¯åˆ¥å¯¾å¿œï¼‰"""
        
        import random
        from datetime import datetime
        
        # æ—¥æœ¬æ™‚é–“ã§ç¾åœ¨ã®æ™‚åˆ»ã‚’å–å¾—
        jst = pytz.timezone('Asia/Tokyo')
        now_jst = datetime.now(jst)
        
        # æ™‚é–“å¸¯ã‚’åˆ¤å®šï¼ˆå¼•æ•°ã§æŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
        if not time_period:
            time_period = "morning" if 6 <= now_jst.hour <= 12 else "evening"
        
        # æ™‚é–“å¸¯åˆ¥ã®ã‚·ãƒ¼ãƒ‰å€¤ã‚’ç”Ÿæˆ
        base_seed = now_jst.day + now_jst.month + now_jst.year
        time_factor = 100 if time_period == "morning" else 200  # æœãƒ»å¤œã§å¤§ããç•°ãªã‚‹å€¤
        weather_factor = (now_jst.day * 7) % 13  # å¤©æ°—ã£ã½ã„ãƒ©ãƒ³ãƒ€ãƒ è¦ç´ 
        season_factor = (now_jst.month - 1) // 3  # å­£ç¯€è¦ç´ ï¼ˆ0-3ï¼‰
        lunar_cycle = now_jst.day % 28  # æœˆé½¢ã£ã½ã„ã‚µã‚¤ã‚¯ãƒ«
        
        # æ™‚é–“å¸¯ã‚’å«ã‚€è¤‡åˆã‚·ãƒ¼ãƒ‰
        random.seed(base_seed + time_factor + weather_factor + season_factor + lunar_cycle)
        
        self.logger.info(f"ğŸ¯ é¸æ‰‹é¸æŠ - æ™‚é–“å¸¯: {time_period}, ã‚·ãƒ¼ãƒ‰: {base_seed + time_factor}")
        
        # æ™‚é–“å¸¯åˆ¥ã®é¸æ‰‹é¸æŠå‚¾å‘
        time_based_focus = {
            'morning': {
                'primary_boost': 'pitchers',    # æœã¯æŠ•æ‰‹é™£ã«æ³¨ç›®
                'secondary_boost': 'catchers'   # ãƒãƒƒãƒ†ãƒªãƒ¼é‡è¦–
            },
            'evening': {
                'primary_boost': 'outfielders', # å¤œã¯é‡æ‰‹é™£ã«æ³¨ç›®
                'secondary_boost': 'infielders' # å†…é‡æ‰‹ã‚‚é‡è¦–
            }
        }
        
        # æ›œæ—¥åˆ¥ã®é¸æ‰‹é¸æŠå‚¾å‘
        weekday_focus = {
            0: 'pitchers',    # æœˆæ›œæ—¥ã¯æŠ•æ‰‹é™£
            1: 'infielders',  # ç«æ›œæ—¥ã¯å†…é‡æ‰‹
            2: 'outfielders', # æ°´æ›œæ—¥ã¯å¤–é‡æ‰‹
            3: 'catchers',    # æœ¨æ›œæ—¥ã¯æ•æ‰‹é™£
            4: 'pitchers',    # é‡‘æ›œæ—¥ã¯æŠ•æ‰‹é™£
            5: 'infielders',  # åœŸæ›œæ—¥ã¯å†…é‡æ‰‹
            6: 'outfielders'  # æ—¥æ›œæ—¥ã¯å¤–é‡æ‰‹
        }
        
        # å­£ç¯€ã«ã‚ˆã‚‹é‡ã¿èª¿æ•´ï¼ˆæ˜¥ã¯æ–°äººã€å¤ã¯ãƒ™ãƒ†ãƒ©ãƒ³ã€ç§‹ã¯ä¸»åŠ›ãªã©ï¼‰
        season_weights = {
            0: {'pitchers': 1.2, 'infielders': 1.0, 'outfielders': 1.0, 'catchers': 0.8},  # æ˜¥
            1: {'pitchers': 1.0, 'infielders': 1.2, 'outfielders': 1.1, 'catchers': 1.0},  # å¤  
            2: {'pitchers': 1.1, 'infielders': 1.1, 'outfielders': 1.2, 'catchers': 1.0},  # ç§‹
            3: {'pitchers': 1.3, 'infielders': 0.9, 'outfielders': 0.9, 'catchers': 1.1}   # å†¬
        }
        
        selected_players = []
        used_positions = []
        
        # æ™‚é–“å¸¯ã«ã‚ˆã‚‹é¸æ‰‹é¸æŠã®å½±éŸ¿
        time_focus = time_based_focus.get(time_period, {'primary_boost': 'pitchers', 'secondary_boost': 'catchers'})
        
        # 1äººç›®ï¼šæ™‚é–“å¸¯é‡è¦– + æ›œæ—¥å‚¾å‘ã®çµ„ã¿åˆã‚ã›
        weekday_position = weekday_focus[now_jst.weekday()]
        time_boost_position = time_focus['primary_boost']
        
        # 40%ã®ç¢ºç‡ã§æ™‚é–“å¸¯é‡è¦–ã€60%ã®ç¢ºç‡ã§æ›œæ—¥é‡è¦–
        if random.random() < 0.4:
            primary_position = time_boost_position
            self.logger.info(f"   ğŸŒ… æ™‚é–“å¸¯é‡è¦–é¸æŠ: {time_boost_position}")
        else:
            primary_position = weekday_position
            self.logger.info(f"   ğŸ“… æ›œæ—¥é‡è¦–é¸æŠ: {weekday_position}")
        
        main_player = random.choice(roster[primary_position])
        selected_players.append(main_player)
        used_positions.append(primary_position)
        
        # 2äººç›®ï¼šåˆ¥ãƒã‚¸ã‚·ãƒ§ãƒ³ã‹ã‚‰é¸æŠ
        available_positions = [pos for pos in roster.keys() if pos != primary_position]
        second_position = random.choice(available_positions)
        
        # ãã®ãƒã‚¸ã‚·ãƒ§ãƒ³ã®é¸æ‰‹ã‹ã‚‰é™¤å¤–ï¼ˆåŒã˜é¸æ‰‹ãŒé¸ã°ã‚Œãªã„ã‚ˆã†ã«ï¼‰
        available_players = [p for p in roster[second_position] if p != main_player]
        second_player = random.choice(available_players)
        selected_players.append(second_player)
        used_positions.append(second_position)
        
        # 3äººç›®ï¼šæ®‹ã‚Šã®ãƒã‚¸ã‚·ãƒ§ãƒ³ã‹ã‚‰é¸æŠï¼ˆå­£ç¯€é‡ã¿ä»˜ãï¼‰
        remaining_positions = [pos for pos in roster.keys() if pos not in used_positions]
        
        # å­£ç¯€é‡ã¿ã‚’é©ç”¨ã—ã¦ãƒã‚¸ã‚·ãƒ§ãƒ³é¸æŠ
        if remaining_positions:
            weights = [season_weights[season_factor].get(pos, 1.0) for pos in remaining_positions]
            third_position = random.choices(remaining_positions, weights=weights, k=1)[0]
        else:
            # å…¨ã¦ã®ãƒã‚¸ã‚·ãƒ§ãƒ³ã‹ã‚‰é¸ã³ç›´ã—
            third_position = random.choice(list(roster.keys()))
        
        # æ—¢ã«é¸ã°ã‚ŒãŸé¸æ‰‹ã‚’é™¤å¤–
        available_players = [p for p in roster[third_position] if p not in selected_players]
        if available_players:
            third_player = random.choice(available_players)
        else:
            # å…¨é¸æ‰‹ã‹ã‚‰é¸ã³ç›´ã—
            all_players = [p for players in roster.values() for p in players if p not in selected_players]
            third_player = random.choice(all_players)
        
        selected_players.append(third_player)
        
        # è¿½åŠ ã®ãƒ©ãƒ³ãƒ€ãƒ è¦ç´ ï¼šç¨€ã«é †ç•ªã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        if random.random() < 0.2:  # 20%ã®ç¢ºç‡ã§é †ç•ªã‚·ãƒ£ãƒƒãƒ•ãƒ«
            random.shuffle(selected_players)
            self.logger.info(f"   ğŸ”€ é †ç•ªã‚·ãƒ£ãƒƒãƒ•ãƒ«å®Ÿè¡Œ")
        
        self.logger.info(f"âœ… æ¨ã—é¸æ‰‹æ±ºå®š: {', '.join(selected_players)} ({time_period})")
        
        return selected_players
    
    async def _fetch_news(self) -> List[str]:
        """RSSçµŒç”±ã§ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
        
        try:
            # RSS ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚’å®Ÿè¡Œ
            rss_collector = BaystarsRSSNewsCollector()
            articles = rss_collector.collect_news_from_multiple_queries(max_articles_per_query=10)
            
            # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã™
            news_titles = [article.get('title', '') for article in articles[:5]]
            
            if news_titles:
                self.logger.info(f"RSSåé›†æˆåŠŸ: {len(news_titles)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—")
                return news_titles
            else:
                self.logger.warning("RSSåé›†: æ–°ã—ã„è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return self._get_fallback_news()
                
        except Exception as e:
            self.logger.error(f"RSSåé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return self._get_fallback_news()
    
    def _get_fallback_news(self) -> List[str]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹"""
        return [
            "ãƒãƒ¼ãƒ ä¸€ä¸¸ã¨ãªã£ã¦ä»Šã‚·ãƒ¼ã‚ºãƒ³ã‚‚é ‘å¼µã£ã¦ã„ã¾ã™",
            "è‹¥æ‰‹é¸æ‰‹ã®æˆé•·ãŒè‘—ã—ãã€æœŸå¾…ãŒé«˜ã¾ã‚Šã¾ã™",
            "ãƒ•ã‚¡ãƒ³ã®çš†æ§˜ã®ç†±ã„å£°æ´ãŒãƒãƒ¼ãƒ ã®åŠ›ã«ãªã£ã¦ã„ã¾ã™",
            "ç·´ç¿’ã«åŠ±ã‚€é¸æ‰‹ãŸã¡ã®å§¿ãŒè¼ã„ã¦ã„ã¾ã™"
        ][:2]
    
    def _get_news_api_key(self) -> str:
        """News API Keyã‚’å–å¾—"""
        
        try:
            ssm_client = boto3.client('ssm')
            parameter_name = os.environ.get('OPENAI_API_KEY_PARAMETER', '/mainichi-homeru/openai-api-key')
            response = ssm_client.get_parameter(
                Name=parameter_name,
                WithDecryption=True
            )
            return response['Parameter']['Value']
        except Exception as e:
            self.logger.error(f"Failed to get News API key: {e}")
            return os.environ.get('NEWS_API_KEY', 'test-key')
    
    async def _search_baystars_news(self, api_key: str = None) -> Dict[str, Any]:
        """Google News RSSçµŒç”±ã§æ¨ã—é¸æ‰‹å€‹åˆ¥ + ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºå…¨ä½“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
        
        try:
            self.logger.info("Starting comprehensive RSS collection for featured players + team news...")
            
            # æ™‚é–“å¸¯ã‚’åˆ¤å®š
            jst = pytz.timezone('Asia/Tokyo')
            now_jst = datetime.now(jst)
            time_period = "morning" if 6 <= now_jst.hour <= 12 else "evening"
            
            # æ¨ã—é¸æ‰‹ã‚’å–å¾—ï¼ˆæ™‚é–“å¸¯åˆ¥é¸æ‰‹æƒ…å ±ã‹ã‚‰ï¼‰
            player_info = await self._fetch_current_players(time_period)
            featured_players = player_info.get('featured_players', ['ä½é‡æµå¤ª', 'ç‰§ç§€æ‚Ÿ', 'æ¾å°¾æ±æ©'])
            
            # æ¨ã—é¸æ‰‹å€‹åˆ¥ + å…¨ä½“ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†
            rss_collector = BaystarsRSSNewsCollector()
            comprehensive_result = rss_collector.collect_comprehensive_news(
                featured_players=featured_players,
                max_per_player=3,
                max_general=5
            )
            
            articles = comprehensive_result['all_articles']
            player_articles = comprehensive_result['player_articles']
            general_articles = comprehensive_result['general_articles']
            
            if articles:
                # è¨˜äº‹ç”Ÿæˆç”¨ã®å½¢å¼ã«å¤‰æ›ï¼ˆæ¨ã—é¸æ‰‹åˆ¥ã«æ•´ç†ï¼‰
                recent_news = []
                trending_players = featured_players  # æ¨ã—é¸æ‰‹ã‚’å„ªå…ˆ
                positive_highlights = []
                
                # æ¨ã—é¸æ‰‹åˆ¥ã®è¨˜äº‹æ•´ç†
                player_news_summary = {}
                for player, player_arts in player_articles.items():
                    if player_arts:
                        player_news_summary[player] = []
                        for article in player_arts:
                            news_item = {
                                'headline': article.get('title', ''),
                                'category': f'é‡çƒãƒ»{player}é¸æ‰‹',
                                'key_facts': [article.get('summary', '')],
                                'mentioned_players': [player],
                                'date': article.get('publish_date', datetime.now().strftime('%Y-%m-%d')),
                                'source': article.get('source', 'Google News'),
                                'query_used': article.get('query_used', ''),
                                'search_type': 'player_specific',
                                'target_player': player
                            }
                            player_news_summary[player].append(news_item)
                            recent_news.append(news_item)
                            
                            # ãƒã‚¸ãƒ†ã‚£ãƒ–ãªè©±é¡Œã‚’æŠ½å‡º
                            title = article.get('title', '')
                            if any(word in title for word in ['å‹åˆ©', 'æ´»èº', 'å¥½èª¿', 'ãƒ›ãƒ¼ãƒ ãƒ©ãƒ³', 'å„ªå‹', 'ãƒ’ãƒƒãƒˆ', 'å¿«å‹', 'å…ˆç™º', 'å¥½æŠ•', 'å¾©æ´»']):
                                positive_highlights.append(f"{player}: {title}")
                
                # å…¨ä½“è¨˜äº‹ã‚‚è¿½åŠ 
                for article in general_articles:
                    news_item = {
                        'headline': article.get('title', ''),
                        'category': 'é‡çƒãƒ»ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º',
                        'key_facts': [article.get('summary', '')],
                        'mentioned_players': article.get('baystars_keywords_found', []),
                        'date': article.get('publish_date', datetime.now().strftime('%Y-%m-%d')),
                        'source': article.get('source', 'Google News'),
                        'query_used': article.get('query_used', ''),
                        'search_type': 'general_team'
                    }
                    recent_news.append(news_item)
                    
                    # ãƒã‚¸ãƒ†ã‚£ãƒ–ãªè©±é¡Œã‚’æŠ½å‡º
                    title = article.get('title', '')
                    if any(word in title for word in ['å‹åˆ©', 'æ´»èº', 'å¥½èª¿', 'ãƒ›ãƒ¼ãƒ ãƒ©ãƒ³', 'å„ªå‹', 'ãƒ’ãƒƒãƒˆ', 'å¿«å‹', 'å…ˆç™º']):
                        positive_highlights.append(title)
                
                game_info = {
                    'has_recent_news': True,
                    'recent_news': recent_news,
                    'trending_players': trending_players,
                    'team_situation': f"æ¨ã—é¸æ‰‹å€‹åˆ¥æ¤œç´¢({len(player_articles)}å) + å…¨ä½“æ¤œç´¢ã§åˆè¨ˆ{len(articles)}ä»¶ã®è¨˜äº‹ã‚’åé›†",
                    'positive_highlights': positive_highlights,
                    'data_source': 'comprehensive_rss',
                    'collection_summary': comprehensive_result['summary'],
                    'player_specific_news': player_news_summary,
                    'general_news': [{'headline': g.get('title', ''), 'source': g.get('source', '')} for g in general_articles],
                    'featured_players': featured_players
                }
                
                self.logger.info(f"Comprehensive RSS collection successful: {len(recent_news)} news items")
                
                # åé›†çµæœã®è©³ç´°ãƒ­ã‚°å‡ºåŠ›
                self.logger.info(f"ğŸ¯ æ¨ã—é¸æ‰‹å€‹åˆ¥æ¤œç´¢çµæœ:")
                for player, player_arts in player_articles.items():
                    self.logger.info(f"  {player}: {len(player_arts)} ä»¶")
                    for i, article in enumerate(player_arts, 1):
                        self.logger.info(f"    [{i}] {article['title'][:50]}... (ã‚½ãƒ¼ã‚¹: {article['source']})")
                
                self.logger.info(f"ğŸ“° å…¨ä½“æ¤œç´¢çµæœ: {len(general_articles)} ä»¶")
                for i, article in enumerate(general_articles[:2], 1):
                    self.logger.info(f"  [{i}] {article['title'][:50]}... (ã‚½ãƒ¼ã‚¹: {article['source']})")
                
                return game_info
            else:
                self.logger.warning("No Baystars news found from RSS, using enhanced fallback")
                return self._get_enhanced_fallback_game_info()
            
        except Exception as e:
            self.logger.error(f"Google News RSS collection failed: {e}")
            import traceback
            self.logger.error(f"Traceback: {traceback.format_exc()}")
            return self._get_fallback_game_info()
    
    
    def _extract_player_names(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é¸æ‰‹åã‚’æŠ½å‡º"""
        
        players = ['ç‰§ç§€æ‚Ÿ', 'ä½é‡æµå¤ª', 'ä¸‰æµ¦å¤§è¼”', 'å®®å´æ•éƒ', 'ä»Šæ°¸æ˜‡å¤ª', 'çŸ³ç”°è£•å¤ªéƒ', 'å±±æœ¬æ‹“å®Ÿ']
        found_players = []
        
        for player in players:
            if player in text:
                found_players.append(player)
        
        return found_players
    
    def _extract_trending_players(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒˆãƒ¬ãƒ³ãƒ‰é¸æ‰‹ã‚’æŠ½å‡º"""
        
        player_count = {}
        players = ['ç‰§', 'ä½é‡', 'ä¸‰æµ¦', 'å®®å´', 'ä»Šæ°¸', 'çŸ³ç”°', 'å±±æœ¬']
        
        for player in players:
            count = text.count(player)
            if count > 0:
                player_count[player] = count
        
        # å‡ºç¾å›æ•°ã§ã‚½ãƒ¼ãƒˆ
        return [player for player, count in sorted(player_count.items(), key=lambda x: x[1], reverse=True)[:3]]
    
    def _find_function_by_name(self, partial_name: str) -> str:
        """é–¢æ•°åã®éƒ¨åˆ†ä¸€è‡´ã§ Lambda é–¢æ•°ã‚’æ¤œç´¢"""
        
        try:
            functions = lambda_client.list_functions()
            for func in functions['Functions']:
                if partial_name in func['FunctionName']:
                    return func['FunctionName']
        except Exception as e:
            self.logger.error(f"Error listing functions: {e}")
        
        return None
    
    def _get_fallback_game_info(self) -> Dict[str, Any]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®è©¦åˆæƒ…å ±"""
        
        return {
            'has_game_today': True,
            'game_result': {
                'opponent': 'ã‚»ãƒªãƒ¼ã‚°ãƒ©ã‚¤ãƒãƒ«',
                'result': 'ç†±æˆ¦',
                'score': 'å¥½ã‚²ãƒ¼ãƒ '
            },
            'recent_news': ['ãƒãƒ¼ãƒ ä¸€ä¸¸ã¨ãªã£ã¦é ‘å¼µã£ã¦ã„ã¾ã™'],
            'player_highlights': ['ä½é‡é¸æ‰‹', 'ç‰§é¸æ‰‹', 'æ¾å°¾é¸æ‰‹']
        }
    
    def _get_enhanced_fallback_game_info(self) -> Dict[str, Any]:
        """å¼·åŒ–ç‰ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®è©¦åˆæƒ…å ±ï¼ˆãƒªã‚¢ãƒ«ãªãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰"""
        
        # å®Ÿéš›ã®ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºã«é–¢ã™ã‚‹æœ€æ–°ãƒˆãƒ”ãƒƒã‚¯ï¼ˆ2025å¹´ã‚·ãƒ¼ã‚ºãƒ³æƒ³å®šï¼‰
        sample_news = [
            {
                'title': 'ç‰§ç§€æ‚Ÿã‚­ãƒ£ãƒ—ãƒ†ãƒ³ãŒé€£æ—¥ã®çŒ›ç·´ç¿’ã§ãƒãƒ¼ãƒ ã‚’ç‰½å¼•',
                'source': 'Yahoo!ã‚¹ãƒãƒ¼ãƒ„(sample)',
                'pattern': 'captain_leadership'
            },
            {
                'title': 'æˆ¸æŸ±æ­å­ãƒ™ãƒ†ãƒ©ãƒ³æ•æ‰‹ãŒè‹¥æ‰‹æŠ•æ‰‹é™£ã®æŒ‡å°ã«ç†±å¿ƒ',
                'source': 'Yahoo!ã‚¹ãƒãƒ¼ãƒ„(sample)', 
                'pattern': 'veteran_guidance'
            },
            {
                'title': 'äº¬ç”°é™½å¤ªã®è¯éº—ãªå®ˆå‚™ãŒãƒ•ã‚¡ãƒ³ã®è©±é¡Œã«',
                'source': 'Yahoo!ã‚¹ãƒãƒ¼ãƒ„(sample)',
                'pattern': 'defensive_play'
            },
            {
                'title': 'ä¸‰æµ¦ç›£ç£ã€Œé¸æ‰‹ãŸã¡ã®æˆé•·ã‚’æ„Ÿã˜ã¦ã„ã‚‹ã€ã¨ã‚³ãƒ¡ãƒ³ãƒˆ',
                'source': 'Yahoo!ã‚¹ãƒãƒ¼ãƒ„(sample)',
                'pattern': 'manager_comment'
            },
            {
                'title': 'ãƒãƒã‚¹ã‚¿ã§ã®ç·´ç¿’ã§é¸æ‰‹ãŸã¡ãŒæ±—ã‚’æµã™',
                'source': 'Yahoo!ã‚¹ãƒãƒ¼ãƒ„(sample)',
                'pattern': 'practice_scene'
            }
        ]
        
        # æ³¨ç›®é¸æ‰‹ï¼ˆå®Ÿåœ¨é¸æ‰‹ï¼‰
        trending_players = ['ç‰§ç§€æ‚Ÿ', 'æˆ¸æŸ±æ­å­', 'äº¬ç”°é™½å¤ª']
        
        # ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒã‚¤ãƒ©ã‚¤ãƒˆ
        positive_highlights = [
            'ã‚­ãƒ£ãƒ—ãƒ†ãƒ³ç‰§ã®çµ±ç‡åŠ›',
            'ãƒ™ãƒ†ãƒ©ãƒ³æˆ¸æŸ±ã®çµŒé¨“',
            'äº¬ç”°ã®å®ˆå‚™åŠ›',
            'ãƒãƒ¼ãƒ ä¸€ä¸¸ã®å–ã‚Šçµ„ã¿',
            'ãƒ•ã‚¡ãƒ³ã¨ã®çµ†'
        ]
        
        return {
            'recent_news': sample_news,
            'trending_players': trending_players,
            'team_situation': f"æœ€æ–°ã®ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºæƒ…å ±{len(sample_news)}ä»¶ã‚’åé›†ï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼‰",
            'positive_highlights': positive_highlights,
            'data_source': 'yahoo_sports_scraping'
        }

class ContentGenerationAgent(MCPAgent):
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self):
        super().__init__("ContentGeneration")
    
    async def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ"""
        
        self.logger.info("Starting content generation...")
        
        collected_data = context.get('collected_data', {})
        
        # æ—¥æœ¬æ™‚é–“ã§ç¾åœ¨ã®æ™‚åˆ»ã¨æ™‚é–“å¸¯ã‚’å–å¾—
        jst = pytz.timezone('Asia/Tokyo')
        now_jst = datetime.now(jst)
        today_jp = now_jst.strftime('%Yå¹´%mæœˆ%dæ—¥')
        current_hour = now_jst.hour
        
        # æ™‚é–“å¸¯ã‚’åˆ¤å®šï¼ˆ9æ™‚é ƒãªã‚‰æœã€21æ™‚é ƒãªã‚‰å¤œï¼‰
        if 6 <= current_hour <= 12:
            time_period = "morning"
            time_greeting = "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™"
        else:
            time_period = "evening"  
            time_greeting = "ãŠç–²ã‚Œæ§˜ã§ã™"
        
        self.logger.info(f"Current JST time: {now_jst.strftime('%H:%M')}, Period: {time_period}")
        
        # 1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼ˆæ™‚é–“å¸¯æƒ…å ±ã‚’è¿½åŠ ï¼‰
        content_context = self._analyze_context(collected_data, time_period, time_greeting)
        
        # 2. è¨˜äº‹ç”Ÿæˆ
        article_content = await self._generate_article(today_jp, content_context)
        
        # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        metadata = self._generate_metadata(article_content, content_context)
        
        self.logger.info("Content generation completed")
        
        return {
            'status': 'success',
            'data': {
                'article_content': article_content,
                'metadata': metadata,
                'context_used': content_context
            },
            'agent': self.name
        }
    
    def _analyze_context(self, collected_data: Dict[str, Any], time_period: str, time_greeting: str) -> Dict[str, Any]:
        """åé›†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰"""
        
        game_info = collected_data.get('game_info', {})
        player_info = collected_data.get('player_info', {})
        news_info = collected_data.get('news_info', [])
        
        # è¨˜äº‹ã®æ–¹å‘æ€§ã‚’æ±ºå®šï¼ˆé¸æ‰‹å¿œæ´è¨˜äº‹ã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ï¼‰
        article_theme = "player_spotlight"  # é¸æ‰‹ã«ã‚¹ãƒãƒƒãƒˆãƒ©ã‚¤ãƒˆã‚’å½“ã¦ãŸè¨˜äº‹
        
        # ä½¿ç”¨ã™ã‚‹é¸æ‰‹ã‚’é¸å®šï¼ˆ3åã«æ‹¡å¼µï¼‰
        featured_players = player_info.get('featured_players', ['ä½é‡æµå¤ª', 'ç‰§ç§€æ‚Ÿ', 'æ¾å°¾æ±æ©'])
        
        return {
            'theme': article_theme,
            'featured_players': featured_players[:3],  # æœ€å¤§3åã«æ‹¡å¼µ
            'news_highlights': news_info[:1],  # æœ€æ–°1ä»¶
            'generation_style': 'player_focused_positive',  # é¸æ‰‹ä¸­å¿ƒã®ãƒã‚¸ãƒ†ã‚£ãƒ–è¨˜äº‹
            'game_context': game_info,  # GPT-4oã‹ã‚‰ã®æƒ…å ±ã‚’è¿½åŠ 
            'time_period': time_period,  # æœ/å¤œã®æ™‚é–“å¸¯æƒ…å ±
            'time_greeting': time_greeting  # æ™‚é–“å¸¯ã«å¿œã˜ãŸæŒ¨æ‹¶
        }
    
    async def _generate_article(self, today_jp: str, context: Dict[str, Any]) -> str:
        """Claude APIã‚’ä½¿ç”¨ã—ã¦è¨˜äº‹ã‚’ç”Ÿæˆ"""
        
        try:
            api_key = self._get_claude_api_key()
            self.logger.info(f"Claude API key retrieved: {api_key[:10]}..." if api_key else "No API key")
            
            if api_key == 'test-key' or not api_key:
                self.logger.warning("Using mock article due to missing/invalid API key")
                return self._generate_mock_article(today_jp, context)
            
            # Anthropicãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ä»£ã‚ã‚Šã«ç›´æ¥HTTP APIã‚’å‘¼ã³å‡ºã—
            import json
            import urllib.request
            import urllib.parse
            
            prompt = self._build_advanced_prompt(today_jp, context)
            self.logger.info(f"Prompt length: {len(prompt)} characters")
            
            # Claude APIç›´æ¥å‘¼ã³å‡ºã—
            url = "https://api.anthropic.com/v1/messages"
            headers = {
                "Content-Type": "application/json",
                "x-api-key": api_key,
                "anthropic-version": "2023-06-01"
            }
            
            data = {
                "model": "claude-3-5-sonnet-20241022",
                "max_tokens": 2000,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            }
            
            req = urllib.request.Request(
                url,
                data=json.dumps(data).encode('utf-8'),
                headers=headers
            )
            
            with urllib.request.urlopen(req) as response:
                result = json.loads(response.read().decode('utf-8'))
                
            generated_content = result['content'][0]['text']
            self.logger.info(f"Claude generated article: {len(generated_content)} characters")
            return generated_content
            
        except Exception as e:
            self.logger.error(f"Error generating article: {e}")
            return self._generate_mock_article(today_jp, context)
    
    def _build_advanced_prompt(self, today_jp: str, context: Dict[str, Any]) -> str:
        """é«˜åº¦ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
        
        theme = context.get('theme', 'general_positive')
        players = context.get('featured_players', ['ä½é‡é¸æ‰‹', 'ç‰§é¸æ‰‹'])
        game_context = context.get('game_context', {})
        news = context.get('news_highlights', [])
        
        # é¸æ‰‹æƒ…å ±ã®æ•´ç†ï¼ˆãƒã‚¸ã‚·ãƒ§ãƒ³ä»˜ãï¼‰
        player_context = self._get_player_context(players, game_context)
        
        # ãƒ†ãƒ¼ãƒåˆ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª¿æ•´ï¼ˆé¸æ‰‹ä¸­å¿ƒï¼‰
        theme_instruction = {
            'player_spotlight': 'é¸æ‰‹ãŸã¡ã®é­…åŠ›ã‚„é ‘å¼µã‚Šã‚’è¤’ã‚ç§°ãˆã‚‹å¿œæ´è¨˜äº‹ã¨ã—ã¦',
            'general_positive': 'æ—¥å¸¸çš„ãªå¿œæ´ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦'
        }.get(theme, 'é¸æ‰‹ãŸã¡ã‚’å¿œæ´ã™ã‚‹æ°—æŒã¡ã‚’è¾¼ã‚ã¦')
        
        # è©¦åˆçµæœã¯ä½¿ã‚ãšã€é¸æ‰‹ã®æƒ…å ±ã®ã¿ã«é›†ä¸­
        
        # GPT-4oã‹ã‚‰åé›†ã—ãŸæœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã‚’çµ±åˆ
        news_context = ""
        recent_news_context = ""
        
        if game_context.get('data_source') == 'gpt4o_news_search':
            # GPT-4oã‹ã‚‰ã®æƒ…å ±ã‚’ä½¿ç”¨
            recent_news = game_context.get('recent_news', [])
            team_situation = game_context.get('team_situation', '')
            positive_highlights = game_context.get('positive_highlights', [])
            
            if recent_news:
                news_items = []
                for news_item in recent_news[:3]:  # æœ€æ–°3ä»¶
                    headline = news_item.get('headline', '')
                    key_facts = ', '.join(news_item.get('key_facts', []))
                    mentioned_players = ', '.join(news_item.get('mentioned_players', []))
                    news_items.append(f"ãƒ»{headline}ï¼ˆ{key_facts}ï¼‰{mentioned_players}")
                
                recent_news_context = f"""
æœ€è¿‘ã®ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºãƒ‹ãƒ¥ãƒ¼ã‚¹:
{chr(10).join(news_items)}

ãƒãƒ¼ãƒ çŠ¶æ³: {team_situation}
æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆ: {', '.join(positive_highlights)}"""
        
        elif game_context.get('data_source') == 'comprehensive_rss':
            # æ¨ã—é¸æ‰‹å€‹åˆ¥æ¤œç´¢ + å…¨ä½“æ¤œç´¢ã®æƒ…å ±ã‚’ä½¿ç”¨
            recent_news = game_context.get('recent_news', [])
            team_situation = game_context.get('team_situation', '')
            positive_highlights = game_context.get('positive_highlights', [])
            trending_players = game_context.get('trending_players', [])
            collection_summary = game_context.get('collection_summary', {})
            player_specific_news = game_context.get('player_specific_news', {})
            general_news = game_context.get('general_news', [])
            featured_players = game_context.get('featured_players', [])
            
            if recent_news:
                # æ¨ã—é¸æ‰‹åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ•´ç†
                player_news_sections = []
                for player in featured_players:
                    player_articles = player_specific_news.get(player, [])
                    if player_articles:
                        player_section = f"""
ã€{player}é¸æ‰‹ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€‘:"""
                        for i, article in enumerate(player_articles[:3], 1):
                            headline = article.get('headline', '')
                            source = article.get('source', '')
                            player_section += f"""
{i}. {headline}ï¼ˆ{source}ï¼‰"""
                        player_news_sections.append(player_section)
                
                # å…¨ä½“ãƒ‹ãƒ¥ãƒ¼ã‚¹
                general_section = ""
                if general_news:
                    general_section = f"""
ã€ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºå…¨ä½“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€‘:"""
                    for i, news in enumerate(general_news[:3], 1):
                        headline = news.get('headline', '')
                        source = news.get('source', '')
                        general_section += f"""
{i}. {headline}ï¼ˆ{source}ï¼‰"""
                
                recent_news_context = f"""
ğŸ¯ æ¨ã—é¸æ‰‹å€‹åˆ¥æ¤œç´¢ + ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºå…¨ä½“æ¤œç´¢çµæœ:
{chr(10).join(player_news_sections)}
{general_section}

ğŸ“Š åé›†ã‚µãƒãƒªãƒ¼:
- æ¨ã—é¸æ‰‹è¨˜äº‹: {collection_summary.get('player_articles_count', 0)} ä»¶
- å…¨ä½“è¨˜äº‹: {collection_summary.get('general_articles_count', 0)} ä»¶
- åˆè¨ˆ: {collection_summary.get('total_articles', 0)} ä»¶

ãƒãƒ¼ãƒ çŠ¶æ³: {team_situation}
æ³¨ç›®é¸æ‰‹: {', '.join(featured_players)}
æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆ: {', '.join(positive_highlights)}"""
        
        elif game_context.get('data_source') == 'google_news_rss':
            # æ—§ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
            recent_news = game_context.get('recent_news', [])
            team_situation = game_context.get('team_situation', '')
            positive_highlights = game_context.get('positive_highlights', [])
            trending_players = game_context.get('trending_players', [])
            collection_summary = game_context.get('collection_summary', '')
            
            if recent_news:
                news_items = []
                for news_item in recent_news[:5]:
                    headline = news_item.get('headline', '')
                    source = news_item.get('source', '')
                    if headline and len(headline) > 10:
                        news_items.append(f"ãƒ»{headline}ï¼ˆ{source}ï¼‰")
                
                recent_news_context = f"""
Google News RSSã‹ã‚‰åé›†ã—ãŸæœ€æ–°ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºæƒ…å ±:
{chr(10).join(news_items)}

ãƒãƒ¼ãƒ çŠ¶æ³: {team_situation}
æ³¨ç›®é¸æ‰‹ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(trending_players)}
æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆ: {', '.join(positive_highlights)}

{collection_summary}"""
        
        elif game_context.get('data_source') == 'yahoo_sports_scraping':
            # Yahoo!ã‚¹ãƒãƒ¼ãƒ„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‹ã‚‰ã®æƒ…å ±ã‚’ä½¿ç”¨ï¼ˆæ—§ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œï¼‰
            recent_news = game_context.get('recent_news', [])
            team_situation = game_context.get('team_situation', '')
            positive_highlights = game_context.get('positive_highlights', [])
            trending_players = game_context.get('trending_players', [])
            
            if recent_news:
                news_items = []
                for news_item in recent_news[:5]:  # æœ€æ–°5ä»¶
                    title = news_item.get('title', '')
                    source = news_item.get('source', '')
                    if title and len(title) > 10:
                        news_items.append(f"ãƒ»{title}ï¼ˆ{source}ï¼‰")
                
                recent_news_context = f"""
Yahoo!ã‚¹ãƒãƒ¼ãƒ„ã‹ã‚‰åé›†ã—ãŸæœ€æ–°ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºæƒ…å ±:
{chr(10).join(news_items)}

ãƒãƒ¼ãƒ çŠ¶æ³: {team_situation}
æ³¨ç›®é¸æ‰‹: {', '.join(trending_players)}
æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆ: {', '.join(positive_highlights)}"""
        
        # å¾“æ¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã‚‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã—ã¦ä½¿ç”¨
        if news and not recent_news_context:
            news_context = f"æœ€è¿‘ã®ãƒãƒ¼ãƒ çŠ¶æ³: {news[0]}"
        
        # æ™‚é–“å¸¯æƒ…å ±ã‚’å–å¾—
        time_period = context.get('time_period', 'morning')
        time_greeting = context.get('time_greeting', 'ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™')
        
        # æ™‚é–“å¸¯ã«å¿œã˜ãŸè¨˜äº‹ã®å°å…¥ã‚’èª¿æ•´
        time_context = {
            'morning': 'æ–°ã—ã„ä¸€æ—¥ã®å§‹ã¾ã‚Šã¨å…±ã«ã€ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºã®è©±é¡Œã‚’ãŠå±Šã‘ã—ã¾ã™',
            'evening': 'ä»Šæ—¥ä¸€æ—¥ã®ç· ã‚ããã‚Šã«ã€ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºã®æœ€æ–°æƒ…å ±ã‚’ãŠæ¥½ã—ã¿ãã ã•ã„'
        }.get(time_period, 'ä»Šæ—¥ã‚‚ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºã®è©±é¡Œã‚’ãŠå±Šã‘ã—ã¾ã™')
        
        prompt = f"""ã‚ãªãŸã¯æ¨ªæµœDeNAãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºã®çŸ¥è­˜è±Šå¯Œã§æƒ…ç†±çš„ãªãƒ•ã‚¡ãƒ³ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
{time_greeting}ï¼{today_jp}ã®ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºã«ã¤ã„ã¦ã€{theme_instruction}1000-1200æ–‡å­—ã®é«˜å“è³ªãªè¨˜äº‹ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚
{time_context}ã€‚

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±:
{player_context}
{recent_news_context if recent_news_context else news_context}

è¨˜äº‹ã®è¦ä»¶:
- Markdownå½¢å¼ï¼ˆ# ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é–‹å§‹ï¼‰
- ãƒ™ãƒ†ãƒ©ãƒ³ã‚¹ãƒãƒ¼ãƒ„ãƒ©ã‚¤ã‚¿ãƒ¼ç´šã®æ–‡ç« åŠ›ã§åŸ·ç­†
- **æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã‚’è¨˜äº‹ã®ä¸»è»¸ã«ã—ã¦æ§‹æˆã—ã¦ãã ã•ã„**
- åé›†ã•ã‚ŒãŸå„ãƒ‹ãƒ¥ãƒ¼ã‚¹é …ç›®ã«ã¤ã„ã¦å…·ä½“çš„ã«è¨€åŠã—ã€è©³ã—ãè§£èª¬
- ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«é–¢é€£ã™ã‚‹é¸æ‰‹ã‚„å‡ºæ¥äº‹ã‚’ä¸­å¿ƒã«è¨˜äº‹ã‚’å±•é–‹
- è©±é¡Œã«ãªã£ã¦ã„ã‚‹ãƒˆãƒ”ãƒƒã‚¯ï¼ˆã‚¹ãƒãƒ³ã‚µãƒ¼å¥‘ç´„ã€ãƒˆãƒ¬ãƒ¼ãƒ‰ã€è©¦åˆæƒ…å ±ãªã©ï¼‰ã‚’è©³ã—ãç´¹ä»‹
- ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®èƒŒæ™¯ã‚„æ„ç¾©ã€ãƒ•ã‚¡ãƒ³ã¸ã®å½±éŸ¿ã‚’åˆ†æçš„ã«è§£èª¬
- æ³¨ç›®é¸æ‰‹ã«ã¤ã„ã¦ã¯ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¨é–¢é€£ä»˜ã‘ã¦è‡ªç„¶ã«ç´¹ä»‹
- ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºãƒ•ã‚¡ãƒ³ãŒçŸ¥ã‚ŠãŸã„æœ€æ–°æƒ…å ±ã‚’ç¶²ç¾…çš„ã«ã‚«ãƒãƒ¼
- è‡¨å ´æ„Ÿã®ã‚ã‚‹æå†™ï¼ˆçƒå ´ã®é›°å›²æ°—ã€ãƒ•ã‚¡ãƒ³ã®åå¿œãªã©ï¼‰
- ãƒãƒã‚¹ã‚¿ã®ç‰¹åˆ¥æ„Ÿã‚„ãƒ•ã‚¡ãƒ³ã®ç†±æ°—ã‚‚è¡¨ç¾
- èª­ã¿å¿œãˆã®ã‚ã‚‹å†…å®¹ã§ã€æœ€æ–°æƒ…å ±ã«åŸºã¥ã„ãŸä¾¡å€¤ã‚ã‚‹è¨˜äº‹

æ–‡ä½“ã®ç‰¹å¾´:
- ãƒ—ãƒ­ã®ã‚¹ãƒãƒ¼ãƒ„ãƒ©ã‚¤ã‚¿ãƒ¼é¢¨ã®çŸ¥çš„ã§æ´—ç·´ã•ã‚ŒãŸè¡¨ç¾
- é©åº¦ãªå°‚é–€ç”¨èªï¼ˆé‡çƒç”¨èªï¼‰ã‚’è‡ªç„¶ã«ç¹”ã‚Šè¾¼ã‚€
- ãƒ•ã‚¡ãƒ³ã®å¿ƒã‚’æ´ã‚€æ„Ÿå‹•çš„ãªãƒ•ãƒ¬ãƒ¼ã‚º
- ãƒ‹ãƒ¥ãƒ¼ã‚¹è§£èª¬ã¨å¿œæ´è¨˜äº‹ã®ãƒãƒ©ãƒ³ã‚¹

é‡è¦ãªæŒ‡ç¤º: 
- **ã€æœ€å„ªå…ˆã€‘æ¨ã—é¸æ‰‹3åã‚’ä¸­å¿ƒã¨ã—ãŸè¨˜äº‹ã«ã—ã¦ãã ã•ã„**
- å„æ¨ã—é¸æ‰‹ã«ã¤ã„ã¦åé›†ã•ã‚ŒãŸæœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è©³ã—ãè§£èª¬ã—ã¦ãã ã•ã„
- æ¨ã—é¸æ‰‹ã®æ´»èºã€æˆé•·ã€åŠªåŠ›ã€é­…åŠ›ã‚’å…·ä½“çš„ã«è¤’ã‚ã¦ãã ã•ã„
- æ¨ã—é¸æ‰‹ã«é–¢ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒæœ€ã‚‚é‡è¦ã§ã€å¿…ãšå…¨å“¡ã«ã¤ã„ã¦è¨€åŠã—ã¦ãã ã•ã„
- ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºå…¨ä½“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚‚è£œå®Œçš„ã«ä½¿ç”¨ã—ã¦ãã ã•ã„
- é¸æ‰‹å€‹äººã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã€ãƒ—ãƒ¬ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ«ã€äººæŸ„ã‚’è©³ã—ãç´¹ä»‹
- é¸æ‰‹ã®æœ€æ–°ã®çŠ¶æ³ã‚„è©±é¡Œã‚’æ­£ç¢ºã«åæ˜ ã—ã¦ãã ã•ã„
- ãƒ•ã‚¡ãƒ³ãŒæ¨ã—é¸æ‰‹ã«ã¤ã„ã¦æ–°ã—ã„æƒ…å ±ã‚’å¾—ã‚‰ã‚Œã‚‹è¨˜äº‹ã«ã—ã¦ãã ã•ã„
- å…·ä½“çš„ãªè©¦åˆçµæœã€ã‚¹ã‚³ã‚¢ã€å‹æ•—ã«ã¯è§¦ã‚Œãªã„ã§ãã ã•ã„
- æ¨ã—é¸æ‰‹3åå…¨å“¡ãŒè¨˜äº‹ã«åæ˜ ã•ã‚Œã¦ã„ãªã„å ´åˆã¯å¤±æ ¼ã§ã™"""
        
        return prompt
    
    def _get_player_context(self, players: List[str], game_context: Dict[str, Any]) -> str:
        """é¸æ‰‹ã®è©³ç´°æƒ…å ±ã‚’å–å¾—ã—ã¦æ–‡è„ˆã‚’æ§‹ç¯‰"""
        
        # 2025å¹´ã‚·ãƒ¼ã‚ºãƒ³ã®å®Ÿéš›ã®é¸æ‰‹æƒ…å ±ï¼ˆä¸€è»ç™»éŒ²é¸æ‰‹ï¼‰
        player_positions = {
            # æŠ•æ‰‹é™£
            'æ±å…‹æ¨¹': 'æŠ•æ‰‹ï¼ˆ11ç•ªãƒ»å…ˆç™ºã‚¨ãƒ¼ã‚¹ï¼‰',
            'ä¼Šå‹¢å¤§å¤¢': 'æŠ•æ‰‹ï¼ˆ13ç•ªãƒ»æŠ‘ãˆï¼‰',
            'å¤§è²«æ™‹ä¸€': 'æŠ•æ‰‹ï¼ˆ16ç•ªãƒ»å…ˆç™ºï¼‰',
            'å‚æœ¬è£•å“‰': 'æŠ•æ‰‹ï¼ˆ20ç•ªãƒ»ä¸­ç¶™ãï¼‰',
            'æ¾æœ¬å‡Œäºº': 'æŠ•æ‰‹ï¼ˆ34ç•ªãƒ»ä¸­ç¶™ãï¼‰',
            'è‹¥æ¾å°šè¼': 'æŠ•æ‰‹ï¼ˆ39ç•ªï¼‰',
            'ã‚¸ãƒ£ã‚¯ã‚½ãƒ³': 'æŠ•æ‰‹ï¼ˆ42ç•ªãƒ»å…ˆç™ºï¼‰',
            'æ£®åŸåº·å¹³': 'æŠ•æ‰‹ï¼ˆ45ç•ªãƒ»ä¸­ç¶™ãï¼‰',
            'é¢¯': 'æŠ•æ‰‹ï¼ˆ53ç•ªï¼‰',
            'çŸ³ç”°è£•å¤ªéƒ': 'æŠ•æ‰‹ï¼ˆ54ç•ªï¼‰',
            'ä¸­å·è™å¤§': 'æŠ•æ‰‹ï¼ˆ64ç•ªï¼‰',
            'å®®åŸæ»å¤ª': 'æŠ•æ‰‹ï¼ˆ65ç•ªãƒ»å…ˆç™ºï¼‰',
            'ã‚±ã‚¤': 'æŠ•æ‰‹ï¼ˆ69ç•ªï¼‰',
            'ãƒã‚¦ã‚¢ãƒ¼': 'æŠ•æ‰‹ï¼ˆ96ç•ªãƒ»å…ˆç™ºï¼‰',
            
            # æ•æ‰‹é™£
            'æ¾å°¾æ±æ©': 'æ•æ‰‹ï¼ˆ5ç•ªãƒ»æ­£æ•æ‰‹ï¼‰',
            'æˆ¸æŸ±æ­å­': 'æ•æ‰‹ï¼ˆ10ç•ªãƒ»ãƒ™ãƒ†ãƒ©ãƒ³ï¼‰',
            'å±±æœ¬ç¥å¤§': 'æ•æ‰‹ï¼ˆ50ç•ªï¼‰',
            
            # å†…é‡æ‰‹é™£
            'æ—ç¢çœŸ': 'å†…é‡æ‰‹ï¼ˆ00ç•ªï¼‰',
            'ç‰§ç§€æ‚Ÿ': 'å†…é‡æ‰‹ï¼ˆ2ç•ªãƒ»ã‚­ãƒ£ãƒ—ãƒ†ãƒ³ãƒ»äºŒå¡æ‰‹ï¼‰',
            'äº¬ç”°é™½å¤ª': 'å†…é‡æ‰‹ï¼ˆ9ç•ªãƒ»éŠæ’ƒæ‰‹ï¼‰',
            'ä¸‰æ£®å¤§è²´': 'å†…é‡æ‰‹ï¼ˆ26ç•ªï¼‰',
            'æŸ´ç”°ç«œæ‹“': 'å†…é‡æ‰‹ï¼ˆ31ç•ªãƒ»ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰',
            'çŸ³ä¸Šæ³°è¼': 'å†…é‡æ‰‹ï¼ˆ44ç•ªï¼‰',
            'å®®ï¨‘æ•éƒ': 'å†…é‡æ‰‹ï¼ˆ51ç•ªãƒ»ä¸‰å¡æ‰‹ãƒ»ãƒ™ãƒ†ãƒ©ãƒ³ï¼‰',
            'äº•ä¸Šçµ¢ç™»': 'å†…é‡æ‰‹ï¼ˆ55ç•ªãƒ»ä¸€å¡æ‰‹ï¼‰',
            'ãƒ•ã‚©ãƒ¼ãƒ‰': 'å†…é‡æ‰‹ï¼ˆ99ç•ªãƒ»ä¸€å¡æ‰‹ï¼‰',
            
            # å¤–é‡æ‰‹é™£
            'æ¡‘åŸå°†å¿—': 'å¤–é‡æ‰‹ï¼ˆ1ç•ªãƒ»ä¸­å …æ‰‹ï¼‰',
            'ä½é‡æµå¤ª': 'å¤–é‡æ‰‹ï¼ˆ7ç•ªãƒ»å·¦ç¿¼æ‰‹ï¼‰',
            'ç¥é‡Œå’Œæ¯…': 'å¤–é‡æ‰‹ï¼ˆ8ç•ªï¼‰',
            'è¦åé”å¤«': 'å¤–é‡æ‰‹ï¼ˆ61ç•ªï¼‰',
            'é–¢æ ¹å¤§æ°—': 'å¤–é‡æ‰‹ï¼ˆ63ç•ªï¼‰'
        }
        
        # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒã‚¤ãƒ©ã‚¤ãƒˆæƒ…å ±ã‹ã‚‰è©³ç´°ã‚’æŠ½å‡º
        player_details = []
        highlights = game_context.get('player_highlights', [])
        
        for player_name in players:
            # é¸æ‰‹åã®æ­£è¦åŒ–ï¼ˆã€Œé¸æ‰‹ã€ã‚’é™¤å»ï¼‰
            clean_name = player_name.replace('é¸æ‰‹', '')
            position = player_positions.get(clean_name, 'é¸æ‰‹')
            
            # ãƒã‚¤ãƒ©ã‚¤ãƒˆæƒ…å ±ã‹ã‚‰è©³ç´°ã‚’å–å¾—
            highlight_info = ""
            for highlight in highlights:
                if clean_name in highlight:
                    if 'ï¼ˆé‡æ‰‹ï¼‰' in highlight:
                        highlight_info = "æ‰“æ’ƒã§æ´»èº"
                    elif 'ï¼ˆæŠ•æ‰‹ï¼‰' in highlight:
                        highlight_info = "æŠ•çƒã§æ´»èº"
                    break
            
            player_details.append(f"{clean_name}é¸æ‰‹ï¼ˆ{position}ï¼‰{f' - {highlight_info}' if highlight_info else ''}")
        
        return f"æ³¨ç›®é¸æ‰‹: {', '.join(player_details)}"
    
    def _generate_mock_article(self, today_jp: str, context: Dict[str, Any]) -> str:
        """ãƒ†ã‚¹ãƒˆç’°å¢ƒç”¨ã®ãƒ¢ãƒƒã‚¯è¨˜äº‹"""
        
        players = ', '.join(context.get('featured_players', ['ä½é‡é¸æ‰‹', 'ç‰§é¸æ‰‹']))
        
        return f"""# ä»Šæ—¥ã‚‚ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºæœ€é«˜ï¼({today_jp})

MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…ç‰ˆã§ã®è¨˜äº‹ç”Ÿæˆã§ã™ï¼

{players}ã®æ´»èºã«æœŸå¾…ãŒé«˜ã¾ã‚Šã¾ã™ã­ã€‚ãƒãƒ¼ãƒ ä¸€ä¸¸ã¨ãªã£ã¦ã€ä»Šæ—¥ã‚‚ç´ æ™´ã‚‰ã—ã„ãƒ—ãƒ¬ãƒ¼ã‚’è¦‹ã›ã¦ãã‚Œã‚‹ã“ã¨ã§ã—ã‚‡ã†ã€‚

ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºãƒ•ã‚¡ãƒ³ã¨ã—ã¦ã€ä»Šæ—¥ã‚‚ä¸€æ—¥é ‘å¼µã‚Šã¾ã—ã‚‡ã†ï¼ ğŸ’ª"""
    
    def _get_claude_api_key(self) -> str:
        """Claude API Keyã‚’å–å¾—"""
        
        try:
            ssm_client = boto3.client('ssm')
            parameter_name = os.environ.get('CLAUDE_API_KEY_PARAMETER', '/mainichi-homeru/claude-api-key')
            response = ssm_client.get_parameter(
                Name=parameter_name,
                WithDecryption=True
            )
            return response['Parameter']['Value']
        except Exception as e:
            self.logger.error(f"Failed to get Claude API key: {e}")
            return os.environ.get('CLAUDE_API_KEY', 'test-key')
    
    def _generate_metadata(self, article_content: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """è¨˜äº‹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        
        # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
        title = "ä»Šæ—¥ã‚‚ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºæœ€é«˜ï¼"
        if article_content.startswith('#'):
            title = article_content.split('\n')[0].replace('#', '').strip()
        
        return {
            'title': title,
            'word_count': len(article_content),
            'featured_players': context.get('featured_players', []),
            'theme': context.get('theme', 'general_positive'),
            'generated_at': datetime.now().isoformat()
        }

class QualityAssuranceAgent(MCPAgent):
    """å“è³ªç®¡ç†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self):
        super().__init__("QualityAssurance")
    
    async def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """è¨˜äº‹ã®å“è³ªã‚’ãƒã‚§ãƒƒã‚¯ãƒ»æ”¹å–„"""
        
        self.logger.info("Starting quality assurance...")
        
        generated_content = context.get('generated_content', {})
        article_content = generated_content.get('article_content', '')
        
        # 1. å“è³ªãƒã‚§ãƒƒã‚¯
        quality_score = self._assess_quality(article_content)
        
        # 2. æ”¹å–„ææ¡ˆ
        improvements = self._suggest_improvements(article_content, quality_score)
        
        # 3. æœ€çµ‚ç‰ˆç”Ÿæˆ
        final_content = self._apply_improvements(article_content, improvements)
        
        self.logger.info(f"Quality assurance completed. Score: {quality_score}/10")
        
        return {
            'status': 'success',
            'data': {
                'final_content': final_content,
                'quality_score': quality_score,
                'improvements_applied': improvements,
                'original_length': len(article_content),
                'final_length': len(final_content)
            },
            'agent': self.name
        }
    
    def _assess_quality(self, content: str) -> float:
        """è¨˜äº‹ã®å“è³ªã‚’è©•ä¾¡ï¼ˆ0-10ã®ã‚¹ã‚³ã‚¢ï¼‰"""
        
        score = 0.0
        
        # åŸºæœ¬ãƒã‚§ãƒƒã‚¯é …ç›®
        if content.startswith('#'):
            score += 1.5  # ã‚¿ã‚¤ãƒˆãƒ«ã‚ã‚Š
        
        # æ–‡å­—æ•°ã«ã‚ˆã‚‹è©•ä¾¡ï¼ˆã‚ˆã‚Šé«˜å“è³ªãªè¨˜äº‹ã‚’æœŸå¾…ï¼‰
        if 800 <= len(content) <= 1500:
            score += 2.5  # ç†æƒ³çš„ãªé•·ã•
        elif 500 <= len(content) < 800:
            score += 1.5  # æœ€ä½é™ã®é•·ã•
        elif len(content) >= 1500:
            score += 2.0  # é•·ã„è¨˜äº‹ã‚‚è©•ä¾¡
        
        if 'ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º' in content:
            score += 1.0  # ãƒãƒ¼ãƒ åè¨€åŠ
        
        # é¸æ‰‹åã®å¹…åºƒã„ãƒã‚§ãƒƒã‚¯ï¼ˆ2025å¹´ãƒ­ã‚¹ã‚¿ãƒ¼ï¼‰
        player_mentions = sum(1 for player in ['ä½é‡', 'ç‰§', 'å®®å´', 'æ¾å°¾', 'æ£®', 'é–¢æ ¹', 'ç¥é‡Œ', 'æ¡‘åŸ', 'ç´°å·', 
                                             'å¤§ç”°', 'çŸ¥é‡', 'æ—', 'ä¸ŠèŒ¶', 'è›¯å', 'æ±', 'ä»Šæ°¸', 'å¤§è²«', 'ã‚¸ãƒ£ã‚¯ã‚½ãƒ³', 
                                             'ãƒã‚¦ã‚¢ãƒ¼', 'çŸ³ç”°', 'å¹³è‰¯', 'å…¥æ±Ÿ', 'ä¼Šå‹¢', 'å±±å´', 'ã‚¦ã‚§ãƒ³ãƒ‡ãƒ«ã‚±ãƒ³'] 
                             if player in content)
        score += min(player_mentions * 0.8, 2.0)  # é¸æ‰‹è¨€åŠï¼ˆè¤‡æ•°ã§ãƒœãƒ¼ãƒŠã‚¹ï¼‰
        
        if content.endswith(('ï¼', 'ã€‚', 'ğŸ’ª', 'âš¾')):
            score += 0.5  # é©åˆ‡ãªçµ‚ã‚ã‚Šæ–¹
        
        # ãƒã‚¸ãƒ†ã‚£ãƒ–è¡¨ç¾ã®å¹…åºƒã„ãƒã‚§ãƒƒã‚¯
        positive_words = ['é ‘å¼µ', 'å¿œæ´', 'æœ€é«˜', 'ç´ æ™´ã‚‰ã—ã„', 'æœŸå¾…', 'é­…åŠ›', 'è¼', 'æ´»èº', 'åŠªåŠ›', 'æƒ…ç†±']
        positive_count = sum(1 for word in positive_words if word in content)
        score += min(positive_count * 0.3, 2.0)
        
        # ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªè¡¨ç¾
        if any(phrase in content for phrase in ['ãƒãƒã‚¹ã‚¿', 'ãƒ—ãƒ¬ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ«', 'ãƒãƒ¼ãƒ ä¸€ä¸¸', 'çƒå ´', 'æ‰“å¸­']):
            score += 1.0  # å°‚é–€çš„ãªè¡¨ç¾
        
        # æ®µè½æ§‹æˆï¼ˆæ”¹è¡Œã«ã‚ˆã‚‹ï¼‰
        paragraphs = len([p for p in content.split('\n\n') if p.strip()])
        if paragraphs >= 3:
            score += 1.0  # è‰¯ã„æ§‹æˆ
        
        return min(score, 10.0)
    
    def _suggest_improvements(self, content: str, score: float) -> List[str]:
        """æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆ"""
        
        improvements = []
        
        if score < 5.0:
            improvements.append("overall_enhancement")
        
        if not content.startswith('#'):
            improvements.append("add_title")
        
        if len(content) < 150:
            improvements.append("expand_content")
        
        if 'ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º' not in content:
            improvements.append("add_team_reference")
        
        return improvements
    
    def _apply_improvements(self, content: str, improvements: List[str]) -> str:
        """æ”¹å–„ã‚’é©ç”¨"""
        
        improved_content = content
        
        if "add_title" in improvements and not content.startswith('#'):
            improved_content = "# ä»Šæ—¥ã‚‚ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºæœ€é«˜ï¼\n\n" + improved_content
        
        if "add_team_reference" in improvements and 'ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º' not in content:
            improved_content = improved_content.replace('ãƒãƒ¼ãƒ ', 'ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º', 1)
        
        if "expand_content" in improvements and len(improved_content) < 150:
            improved_content += "\n\nä»Šæ—¥ã‚‚ä¸€æ—¥ã€ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºã¨å…±ã«é ‘å¼µã‚Šã¾ã—ã‚‡ã†ï¼"
        
        return improved_content

class MCPOrchestrator:
    """MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    
    def __init__(self):
        self.logger = logging.getLogger("MCP.Orchestrator")
        self.agents = {
            'data_collection': DataCollectionAgent(),
            'content_generation': ContentGenerationAgent(),
            'quality_assurance': QualityAssuranceAgent()
        }
    
    async def execute_pipeline(self) -> Dict[str, Any]:
        """MCPãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ"""
        
        self.logger.info("Starting MCP pipeline execution...")
        
        try:
            # Phase 1: ãƒ‡ãƒ¼ã‚¿åé›†
            self.logger.info("Phase 1: Data Collection")
            data_result = await self.agents['data_collection'].execute({})
            
            if data_result['status'] != 'success':
                raise Exception("Data collection failed")
            
            # Phase 2: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ
            self.logger.info("Phase 2: Content Generation")
            content_context = {'collected_data': data_result['data']}
            content_result = await self.agents['content_generation'].execute(content_context)
            
            if content_result['status'] != 'success':
                raise Exception("Content generation failed")
            
            # Phase 3: å“è³ªç®¡ç†
            self.logger.info("Phase 3: Quality Assurance")
            qa_context = {'generated_content': content_result['data']}
            qa_result = await self.agents['quality_assurance'].execute(qa_context)
            
            if qa_result['status'] != 'success':
                raise Exception("Quality assurance failed")
            
            # æœ€çµ‚çµæœ
            final_result = {
                'status': 'success',
                'pipeline_execution_time': datetime.now().isoformat(),
                'phases': {
                    'data_collection': data_result,
                    'content_generation': content_result,
                    'quality_assurance': qa_result
                },
                'final_article': qa_result['data']['final_content'],
                'quality_score': qa_result['data']['quality_score']
            }
            
            self.logger.info(f"MCP pipeline completed successfully. Quality score: {qa_result['data']['quality_score']}")
            
            return final_result
            
        except Exception as e:
            self.logger.error(f"MCP pipeline failed: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }

def lambda_handler(event, context):
    """Lambda ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    
    logger.info(f"MCP Orchestrator Event: {json.dumps(event)}")
    
    try:
        # MCPã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’å®Ÿè¡Œ
        orchestrator = MCPOrchestrator()
        
        # éåŒæœŸå®Ÿè¡Œã‚’ãƒ©ãƒƒãƒ—
        import asyncio
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        result = loop.run_until_complete(orchestrator.execute_pipeline())
        
        if result['status'] == 'success':
            # S3ã«è¨˜äº‹ã‚’ä¿å­˜ï¼ˆæ™‚é–“å¸¯åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«åï¼‰
            jst = pytz.timezone('Asia/Tokyo')
            now_jst = datetime.now(jst)
            today = now_jst.strftime('%Y-%m-%d')
            time_suffix = "morning" if 6 <= now_jst.hour <= 12 else "evening"
            
            bucket_name = os.environ['S3_BUCKET_NAME']
            
            s3_client.put_object(
                Bucket=bucket_name,
                Key=f'articles/{today}-{time_suffix}.md',
                Body=result['final_article'].encode('utf-8'),
                ContentType='text/markdown',
                Metadata={
                    'generated-by': 'mcp-orchestrator',
                    'quality-score': str(result['quality_score']),
                    'generation-time': result['pipeline_execution_time'],
                    'time-period': time_suffix
                }
            )
            
            logger.info(f"MCP article saved to S3: {today}")
            
            return {
                'statusCode': 200,
                'body': json.dumps({
                    'message': 'MCP article generated successfully',
                    'date': today,
                    'quality_score': result['quality_score'],
                    'preview': result['final_article'][:100] + '...'
                }, ensure_ascii=False)
            }
        else:
            return {
                'statusCode': 500,
                'body': json.dumps({
                    'error': 'MCP pipeline failed',
                    'details': result.get('error', 'Unknown error')
                })
            }
    
    except Exception as e:
        logger.error(f"MCP Orchestrator error: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': 'MCP orchestrator failed',
                'message': str(e)
            })
        }