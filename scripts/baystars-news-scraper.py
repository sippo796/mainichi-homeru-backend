#!/usr/bin/env python3
"""
ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºãƒ‹ãƒ¥ãƒ¼ã‚¹è©³ç´°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
Yahoo!ã‚¹ãƒãƒ¼ãƒ„ã‹ã‚‰æŠ½å‡ºã—ãŸãƒªãƒ³ã‚¯ã‚’è¾¿ã£ã¦è¨˜äº‹ã®è©³ç´°æƒ…å ±ã‚’å–å¾—ãƒ»ä¿å­˜ã™ã‚‹
"""

import requests
import json
import re
import time
import hashlib
from datetime import datetime, timedelta
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from pathlib import Path

class BaystarsNewsScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.data_dir = Path(__file__).parent / "scraped_data"
        self.data_dir.mkdir(exist_ok=True)
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã®ãƒãƒƒã‚·ãƒ¥ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«
        self.seen_articles_file = self.data_dir / "seen_articles.json"
        self.seen_articles = self._load_seen_articles()
        
        # ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.baystars_keywords = [
            'ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º', 'DeNA', 'æ¨ªæµœ', 'baystars', 'dena', 'yokohama',
            'ä½é‡', 'ç‰§', 'å®®å´', 'æ£®', 'é–¢æ ¹', 'æ¡‘åŸ', 'å¤§å’Œ', 'ä¸‰æµ¦',
            'ä»Šæ°¸', 'å¤§è²«', 'æ±', 'ä¸ŠèŒ¶è°·', 'å¹³è‰¯', 'å…¥æ±Ÿ', 'ä¼Šå‹¢'
        ]

    def _load_seen_articles(self):
        """æ—¢ã«å‡¦ç†æ¸ˆã¿ã®è¨˜äº‹ãƒãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿"""
        try:
            if self.seen_articles_file.exists():
                with open(self.seen_articles_file, 'r', encoding='utf-8') as f:
                    return set(json.load(f))
        except Exception as e:
            print(f"âš ï¸  æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return set()

    def _save_seen_articles(self):
        """å‡¦ç†æ¸ˆã¿è¨˜äº‹ãƒãƒƒã‚·ãƒ¥ã‚’ä¿å­˜"""
        try:
            with open(self.seen_articles_file, 'w', encoding='utf-8') as f:
                json.dump(list(self.seen_articles), f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _generate_article_hash(self, url, title):
        """è¨˜äº‹ã®ä¸€æ„ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ"""
        content = f"{url}#{title}".encode('utf-8')
        return hashlib.md5(content).hexdigest()

    def _is_baystars_related(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆãŒãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£ã‹ãƒã‚§ãƒƒã‚¯"""
        if not text:
            return False
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in self.baystars_keywords)

    def extract_article_content(self, url):
        """ãƒªãƒ³ã‚¯å…ˆã®è¨˜äº‹å†…å®¹ã‚’è©³ç´°ã«æŠ½å‡º"""
        try:
            print(f"ğŸ“° è¨˜äº‹å–å¾—ä¸­: {url}")
            
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡º
            article_data = {
                'url': url,
                'scraped_at': datetime.now().isoformat(),
                'title': '',
                'content': '',
                'summary': '',
                'publish_date': '',
                'author': '',
                'tags': [],
                'images': [],
                'is_baystars_related': False,
                'baystars_keywords_found': []
            }
            
            # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
            title_selectors = [
                'h1',
                '.article-title',
                '.news-title', 
                '[class*="title"]',
                'title'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem and title_elem.get_text().strip():
                    article_data['title'] = title_elem.get_text().strip()
                    break
            
            # æœ¬æ–‡æŠ½å‡º
            content_selectors = [
                '.article-body',
                '.news-body',
                '.content',
                '[class*="body"]',
                '[class*="content"]',
                'article p',
                '.text'
            ]
            
            content_parts = []
            for selector in content_selectors:
                elements = soup.select(selector)
                for elem in elements:
                    text = elem.get_text().strip()
                    if len(text) > 50:  # æ„å‘³ã®ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ã¿
                        content_parts.append(text)
                
                if content_parts:  # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’ä½¿ç”¨
                    break
            
            article_data['content'] = '\n\n'.join(content_parts)
            
            # è¦ç´„ä½œæˆï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰
            if article_data['content']:
                article_data['summary'] = article_data['content'][:200] + '...' if len(article_data['content']) > 200 else article_data['content']
            
            # å…¬é–‹æ—¥æ™‚æŠ½å‡º
            date_selectors = [
                '[class*="date"]',
                '[class*="time"]',
                'time',
                '.publish-date'
            ]
            
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem:
                    date_text = date_elem.get_text().strip()
                    if re.search(r'\d{4}[-/]\d{1,2}[-/]\d{1,2}', date_text):
                        article_data['publish_date'] = date_text
                        break
            
            # ç”»åƒæŠ½å‡º
            img_elements = soup.select('img[src]')
            for img in img_elements[:5]:  # æœ€å¤§5æš
                src = img.get('src', '')
                if src and not src.startswith('data:'):
                    full_url = urljoin(url, src)
                    article_data['images'].append(full_url)
            
            # ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£åˆ¤å®š
            full_text = f"{article_data['title']} {article_data['content']}"
            article_data['is_baystars_related'] = self._is_baystars_related(full_text)
            
            # è¦‹ã¤ã‹ã£ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨˜éŒ²
            found_keywords = []
            for keyword in self.baystars_keywords:
                if keyword.lower() in full_text.lower():
                    found_keywords.append(keyword)
            article_data['baystars_keywords_found'] = found_keywords
            
            return article_data
            
        except Exception as e:
            print(f"âŒ è¨˜äº‹æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return None

    def process_links(self, links, max_articles=10):
        """ãƒªãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆã‚’å‡¦ç†ã—ã¦è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        print(f"ğŸ”„ {len(links)} ä»¶ã®ãƒªãƒ³ã‚¯ã‚’å‡¦ç†é–‹å§‹...")
        
        processed_articles = []
        new_articles_count = 0
        
        for i, link in enumerate(links, 1):
            if new_articles_count >= max_articles:
                print(f"ğŸ“Š æœ€å¤§è¨˜äº‹æ•° ({max_articles}) ã«é”ã—ã¾ã—ãŸ")
                break
                
            url = link.get('url', '')
            title = link.get('text', '')
            
            if not url:
                continue
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            article_hash = self._generate_article_hash(url, title)
            if article_hash in self.seen_articles:
                print(f"   [{i:2d}] â­ï¸  ã‚¹ã‚­ãƒƒãƒ— (æ—¢å‡¦ç†): {title[:50]}...")
                continue
            
            print(f"   [{i:2d}] ğŸ†• å‡¦ç†ä¸­: {title[:50]}...")
            
            # è¨˜äº‹å†…å®¹å–å¾—
            article_data = self.extract_article_content(url)
            
            if article_data and article_data['is_baystars_related']:
                # æ–°ã—ã„è¨˜äº‹ã¨ã—ã¦è¨˜éŒ²
                self.seen_articles.add(article_hash)
                article_data['hash'] = article_hash
                article_data['source_link_text'] = title
                
                processed_articles.append(article_data)
                new_articles_count += 1
                
                print(f"        âœ… ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£è¨˜äº‹ã¨ã—ã¦ä¿å­˜")
                print(f"        ğŸ·ï¸  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(article_data['baystars_keywords_found'][:3])}")
                
                # ä¿å­˜
                self._save_article(article_data)
                
            else:
                print(f"        â­ï¸  ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£ã§ã¯ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            time.sleep(1)
        
        # å‡¦ç†æ¸ˆã¿ãƒãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
        self._save_seen_articles()
        
        print(f"\nğŸ“Š å‡¦ç†å®Œäº†: {new_articles_count} ä»¶ã®æ–°ã—ã„ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºè¨˜äº‹ã‚’å–å¾—")
        return processed_articles

    def _save_article(self, article_data):
        """å€‹åˆ¥è¨˜äº‹ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆï¼ˆæ—¥ä»˜ + ãƒãƒƒã‚·ãƒ¥ï¼‰
            date_str = datetime.now().strftime('%Y%m%d')
            filename = f"{date_str}_{article_data['hash'][:8]}.json"
            filepath = self.data_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(article_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âš ï¸  è¨˜äº‹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def get_recent_articles(self, days=7):
        """æœ€è¿‘ä¿å­˜ã•ã‚ŒãŸè¨˜äº‹ã‚’å–å¾—"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days)
            recent_articles = []
            
            for json_file in self.data_dir.glob("*.json"):
                if json_file.name == "seen_articles.json":
                    continue
                    
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        article = json.load(f)
                        
                    scraped_at = datetime.fromisoformat(article.get('scraped_at', ''))
                    if scraped_at >= cutoff_date:
                        recent_articles.append(article)
                        
                except Exception as e:
                    print(f"âš ï¸  è¨˜äº‹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({json_file}): {e}")
            
            # æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆ
            recent_articles.sort(key=lambda x: x.get('scraped_at', ''), reverse=True)
            return recent_articles
            
        except Exception as e:
            print(f"âŒ æœ€è¿‘ã®è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•° - test-yahoo-scraping.pyã‹ã‚‰ã®ãƒªãƒ³ã‚¯ã‚’ä½¿ç”¨"""
    scraper = BaystarsNewsScraper()
    
    print("ğŸŸï¸  ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºãƒ‹ãƒ¥ãƒ¼ã‚¹è©³ç´°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼")
    print("=" * 60)
    
    # ã¾ãšåŸºæœ¬çš„ãªãƒªãƒ³ã‚¯æŠ½å‡ºã‚’å®Ÿè¡Œ
    print("ğŸ“¡ Yahoo!ã‚¹ãƒãƒ¼ãƒ„ã‹ã‚‰ãƒªãƒ³ã‚¯æŠ½å‡ºä¸­...")
    
    try:
        # test-yahoo-scraping.pyã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ãƒªãƒ³ã‚¯å–å¾—
        from pathlib import Path
        import sys
        sys.path.append(str(Path(__file__).parent))
        
        # ç›´æ¥Yahoo!ã‹ã‚‰ãƒªãƒ³ã‚¯æŠ½å‡º
        url = "https://baseball.yahoo.co.jp/npb/teams/3/"
        response = scraper.session.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ãƒªãƒ³ã‚¯æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆãƒ­ã‚¸ãƒƒã‚¯ä½¿ç”¨ï¼‰
        links = soup.find_all('a', href=True)
        relevant_links = []
        
        baystars_link_keywords = [
            'ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º', 'dena', 'æ¨ªæµœ', 'baystars',
            'ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'news', 'è©¦åˆ', 'game', 'é¸æ‰‹', 'player',
            'å‹åˆ©', 'æ•—æˆ¦', 'çµæœ', 'é€Ÿå ±', 'æˆç¸¾', 'ã‚¹ã‚¿ãƒ¡ãƒ³'
        ]
        
        for link in links:
            href = link.get('href', '')
            text = link.get_text().strip()
            text_lower = text.lower()
            
            if (any(keyword in text_lower for keyword in baystars_link_keywords) or 
                '/npb/teams/3/' in href):
                if text and len(text) < 80:
                    full_url = urljoin(url, href)
                    relevant_links.append({'text': text, 'url': full_url})
        
        print(f"âœ… {len(relevant_links)} ä»¶ã®ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹")
        
        # ãƒªãƒ³ã‚¯ã‚’å‡¦ç†
        articles = scraper.process_links(relevant_links, max_articles=5)
        
        # çµæœè¡¨ç¤º
        if articles:
            print(f"\nğŸ‰ {len(articles)} ä»¶ã®ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºè¨˜äº‹ã‚’è©³ç´°å–å¾—ï¼")
            
            for i, article in enumerate(articles, 1):
                print(f"\nğŸ“° è¨˜äº‹ {i}:")
                print(f"   ã‚¿ã‚¤ãƒˆãƒ«: {article['title'][:60]}...")
                print(f"   URL: {article['url']}")
                print(f"   è¦ç´„: {article['summary'][:100]}...")
                print(f"   ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(article['baystars_keywords_found'][:3])}")
                
        else:
            print("\nğŸ“­ æ–°ã—ã„è¨˜äº‹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        # æœ€è¿‘ã®è¨˜äº‹çµ±è¨ˆ
        recent = scraper.get_recent_articles(days=7)
        print(f"\nğŸ“Š éå»7æ—¥é–“ã®è¨˜äº‹: {len(recent)} ä»¶")
        
        return 0
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())