#!/usr/bin/env python3
"""
Google News RSSçµŒç”±ã§ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã™ã‚‹é«˜é€Ÿã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
Yahoo!ã‚¹ãƒãƒ¼ãƒ„ã®JavaScriptå•é¡Œã‚’å›é¿ã—ã€ã‚ˆã‚Šå®‰å®šã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚’å®Ÿç¾
"""

import feedparser
import requests
import json
import re
import time
import hashlib
from datetime import datetime, timedelta
from urllib.parse import urljoin, urlparse, quote
from bs4 import BeautifulSoup
from pathlib import Path

class BaystarsRSSNewsCollector:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.data_dir = Path(__file__).parent / "rss_scraped_data"
        self.data_dir.mkdir(exist_ok=True)
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã®ãƒãƒƒã‚·ãƒ¥ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«
        self.seen_articles_file = self.data_dir / "seen_rss_articles.json"
        self.seen_articles = self._load_seen_articles()
        
        # ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆé‡è¤‡ã‚’æ¸›ã‚‰ã—ãŸç‰¹åŒ–å‹ï¼‰
        self.search_queries = [
            "æ¨ªæµœDeNAãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º",
            "æ¨ªæµœDeNAãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º è©¦åˆ"
        ]
        
        # ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        self.baystars_keywords = [
            'ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º', 'DeNA', 'æ¨ªæµœ', 'baystars', 'dena', 'yokohama',
            'ä½é‡', 'ç‰§', 'å®®å´', 'æ£®', 'é–¢æ ¹', 'æ¡‘åŸ', 'å¤§å’Œ', 'ä¸‰æµ¦',
            'ä»Šæ°¸', 'å¤§è²«', 'æ±', 'ä¸ŠèŒ¶è°·', 'å¹³è‰¯', 'å…¥æ±Ÿ', 'ä¼Šå‹¢', 'å±±å´',
            'æˆ¸æŸ±', 'ç›Šå­', 'è¦å', 'äº¬ç”°', 'æ¾å°¾', 'çŸ³å·',
            'è—¤æµª', 'ãƒ“ã‚·ã‚¨ãƒ‰', 'æ©‹æœ¬é”å¼¥'
        ]

    def _load_seen_articles(self):
        """æ—¢ã«å‡¦ç†æ¸ˆã¿ã®è¨˜äº‹ãƒãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿"""
        try:
            if self.seen_articles_file.exists():
                with open(self.seen_articles_file, 'r', encoding='utf-8') as f:
                    return set(json.load(f))
        except Exception as e:
            print(f"âš ï¸  æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return set()

    def _save_seen_articles(self):
        """å‡¦ç†æ¸ˆã¿è¨˜äº‹ãƒãƒƒã‚·ãƒ¥ã‚’ä¿å­˜"""
        try:
            with open(self.seen_articles_file, 'w', encoding='utf-8') as f:
                json.dump(list(self.seen_articles), f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _generate_article_hash(self, url, title):
        """è¨˜äº‹ã®ä¸€æ„ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ"""
        content = f"{url}#{title}".encode('utf-8')
        return hashlib.md5(content).hexdigest()

    def _is_baystars_related(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆãŒãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£ã‹ãƒã‚§ãƒƒã‚¯"""
        if not text:
            return False
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in self.baystars_keywords)

    def fetch_rss_feed(self, query):
        """Google News RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—"""
        try:
            # Google News RSS URLæ§‹ç¯‰
            encoded_query = quote(query)
            rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ja&gl=JP&ceid=JP:ja"
            
            print(f"ğŸ“¡ RSSå–å¾—ä¸­: {query}")
            
            # feedparserã§RSSã‚’è§£æ
            feed = feedparser.parse(rss_url)
            
            if feed.bozo:
                print(f"âš ï¸  RSSè§£æè­¦å‘Š: {feed.bozo_exception}")
            
            return feed
            
        except Exception as e:
            print(f"âŒ RSSå–å¾—ã‚¨ãƒ©ãƒ¼ ({query}): {e}")
            return None

    def extract_article_from_rss_item(self, item):
        """RSS itemã‹ã‚‰è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡º"""
        try:
            # åŸºæœ¬æƒ…å ±ã‚’RSSã‹ã‚‰å–å¾—
            article_data = {
                'title': item.get('title', ''),
                'url': item.get('link', ''),
                'summary': item.get('summary', ''),
                'publish_date': item.get('published', ''),
                'source': item.get('source', {}).get('title', ''),
                'scraped_at': datetime.now().isoformat(),
                'content': '',
                'images': [],
                'is_baystars_related': False,
                'baystars_keywords_found': [],
                'collection_method': 'google_news_rss'
            }
            
            # HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã¦ç´”ç²‹ãªãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            if article_data['summary']:
                soup = BeautifulSoup(article_data['summary'], 'html.parser')
                article_data['summary'] = soup.get_text().strip()
            
            # ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£åˆ¤å®š
            full_text = f"{article_data['title']} {article_data['summary']}"
            article_data['is_baystars_related'] = self._is_baystars_related(full_text)
            
            # è¦‹ã¤ã‹ã£ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨˜éŒ²
            found_keywords = []
            for keyword in self.baystars_keywords:
                if keyword.lower() in full_text.lower():
                    found_keywords.append(keyword)
            article_data['baystars_keywords_found'] = found_keywords
            
            # Google Newsã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã‹ã‚‰å®Ÿéš›ã®URLã‚’æŠ½å‡º
            if '/rss/articles/' in article_data['url']:
                print(f"   ğŸ“° Google Newsãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆè¨˜äº‹: {article_data['title'][:50]}...")
                # å®Ÿéš›ã®è¨˜äº‹å–å¾—ã¯é‡ã„ã®ã§ã€RSSæƒ…å ±ã®ã¿ä½¿ç”¨
            
            return article_data
            
        except Exception as e:
            print(f"âŒ RSSè¨˜äº‹æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def collect_news_from_multiple_queries(self, max_articles_per_query=10):
        """è¤‡æ•°ã®ã‚¯ã‚¨ãƒªã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
        print(f"ğŸ”„ {len(self.search_queries)} ç¨®é¡ã®ã‚¯ã‚¨ãƒªã§RSSãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹...")
        
        all_articles = []
        new_articles_count = 0
        
        for i, query in enumerate(self.search_queries, 1):
            print(f"\nğŸ“‹ ã‚¯ã‚¨ãƒª {i}/{len(self.search_queries)}: {query}")
            
            # RSSãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—
            feed = self.fetch_rss_feed(query)
            if not feed or not hasattr(feed, 'entries'):
                print(f"   âŒ ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—å¤±æ•—")
                continue
            
            print(f"   ğŸ“Š RSSè¨˜äº‹æ•°: {len(feed.entries)} ä»¶")
            
            query_articles = []
            
            for j, item in enumerate(feed.entries[:max_articles_per_query], 1):
                title = item.get('title', '')
                url = item.get('link', '')
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                article_hash = self._generate_article_hash(url, title)
                if article_hash in self.seen_articles:
                    print(f"   [{j:2d}] â­ï¸  ã‚¹ã‚­ãƒƒãƒ— (æ—¢å‡¦ç†): {title[:40]}...")
                    continue
                
                print(f"   [{j:2d}] ğŸ†• å‡¦ç†ä¸­: {title[:40]}...")
                
                # RSSè¨˜äº‹ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
                article_data = self.extract_article_from_rss_item(item)
                
                if article_data and article_data['is_baystars_related']:
                    # æ–°ã—ã„è¨˜äº‹ã¨ã—ã¦è¨˜éŒ²
                    self.seen_articles.add(article_hash)
                    article_data['hash'] = article_hash
                    article_data['query_used'] = query
                    
                    query_articles.append(article_data)
                    all_articles.append(article_data)
                    new_articles_count += 1
                    
                    print(f"        âœ… ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£è¨˜äº‹ã¨ã—ã¦åé›†")
                    print(f"        ğŸ·ï¸  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(article_data['baystars_keywords_found'][:3])}")
                    print(f"        ğŸ“° ã‚½ãƒ¼ã‚¹: {article_data['source']}")
                    
                    # ä¿å­˜
                    self._save_article(article_data)
                    
                else:
                    print(f"        â­ï¸  ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºé–¢é€£ã§ã¯ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            
            print(f"   ğŸ“Š ã‚¯ã‚¨ãƒªçµæœ: {len(query_articles)} ä»¶ã®æ–°è¨˜äº‹")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆGoogle News APIã«å„ªã—ãï¼‰
            time.sleep(2)
        
        # å‡¦ç†æ¸ˆã¿ãƒãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
        self._save_seen_articles()
        
        print(f"\nğŸ“Š RSSåé›†å®Œäº†: {new_articles_count} ä»¶ã®æ–°ã—ã„ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºè¨˜äº‹ã‚’å–å¾—")
        return all_articles

    def _save_article(self, article_data):
        """å€‹åˆ¥è¨˜äº‹ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆï¼ˆæ—¥ä»˜ + ãƒãƒƒã‚·ãƒ¥ï¼‰
            date_str = datetime.now().strftime('%Y%m%d')
            filename = f"rss_{date_str}_{article_data['hash'][:8]}.json"
            filepath = self.data_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(article_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âš ï¸  è¨˜äº‹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def get_recent_articles(self, days=7):
        """æœ€è¿‘ä¿å­˜ã•ã‚ŒãŸè¨˜äº‹ã‚’å–å¾—"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days)
            recent_articles = []
            
            for json_file in self.data_dir.glob("*.json"):
                if json_file.name == "seen_rss_articles.json":
                    continue
                    
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        article = json.load(f)
                        
                    scraped_at = datetime.fromisoformat(article.get('scraped_at', ''))
                    if scraped_at >= cutoff_date:
                        recent_articles.append(article)
                        
                except Exception as e:
                    print(f"âš ï¸  è¨˜äº‹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({json_file}): {e}")
            
            # æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆ
            recent_articles.sort(key=lambda x: x.get('scraped_at', ''), reverse=True)
            return recent_articles
            
        except Exception as e:
            print(f"âŒ æœ€è¿‘ã®è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def generate_summary_report(self, articles):
        """åé›†çµæœã®ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if not articles:
            return "ğŸ“­ æ–°ã—ã„è¨˜äº‹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
        
        # çµ±è¨ˆæƒ…å ±
        total_articles = len(articles)
        sources = {}
        keywords_count = {}
        
        for article in articles:
            # ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ
            source = article.get('source', 'Unknown')
            sources[source] = sources.get(source, 0) + 1
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¥çµ±è¨ˆ
            for keyword in article.get('baystars_keywords_found', []):
                keywords_count[keyword] = keywords_count.get(keyword, 0) + 1
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = [
            f"ğŸ“Š ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºRSSãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ãƒ¬ãƒãƒ¼ãƒˆ",
            f"=" * 50,
            f"ğŸ¯ æ–°è¦è¨˜äº‹æ•°: {total_articles} ä»¶",
            f"ğŸ“° æƒ…å ±ã‚½ãƒ¼ã‚¹æ•°: {len(sources)} ã‚µã‚¤ãƒˆ",
            "",
            "ğŸ“ˆ ä¸»è¦ã‚½ãƒ¼ã‚¹:",
        ]
        
        # ã‚½ãƒ¼ã‚¹åˆ¥è¡¨ç¤ºï¼ˆä¸Šä½5ã¤ï¼‰
        sorted_sources = sorted(sources.items(), key=lambda x: x[1], reverse=True)
        for source, count in sorted_sources[:5]:
            report.append(f"   {source}: {count} ä»¶")
        
        report.extend([
            "",
            "ğŸ·ï¸  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:"
        ])
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¥è¡¨ç¤ºï¼ˆä¸Šä½10å€‹ï¼‰
        sorted_keywords = sorted(keywords_count.items(), key=lambda x: x[1], reverse=True)
        for keyword, count in sorted_keywords[:10]:
            report.append(f"   {keyword}: {count} å›")
        
        return "\n".join(report)

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    collector = BaystarsRSSNewsCollector()
    
    print("ğŸŸï¸  ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºRSSãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼")
    print("=" * 60)
    print("ğŸ“¡ Google News RSSçµŒç”±ã§ãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚ºãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é«˜é€Ÿåé›†")
    print("")
    
    try:
        # RSS ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å®Ÿè¡Œ
        articles = collector.collect_news_from_multiple_queries(max_articles_per_query=15)
        
        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
        print(f"\n{collector.generate_summary_report(articles)}")
        
        if articles:
            print(f"\nğŸ“° æœ€æ–°è¨˜äº‹ã‚µãƒ³ãƒ—ãƒ«:")
            print("-" * 60)
            
            for i, article in enumerate(articles[:3], 1):
                print(f"è¨˜äº‹ {i}:")
                print(f"   ã‚¿ã‚¤ãƒˆãƒ«: {article['title'][:60]}...")
                print(f"   ã‚½ãƒ¼ã‚¹: {article['source']}")
                print(f"   å…¬é–‹æ—¥: {article['publish_date']}")
                print(f"   ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(article['baystars_keywords_found'][:5])}")
                print(f"   è¦ç´„: {article['summary'][:100]}...")
                print("")
        
        # æœ€è¿‘ã®è¨˜äº‹çµ±è¨ˆ
        recent = collector.get_recent_articles(days=7)
        print(f"ğŸ“Š éå»7æ—¥é–“ã®è¨˜äº‹: {len(recent)} ä»¶")
        
        return 0
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        return 1
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())